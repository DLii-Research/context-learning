{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Source\n",
    "\n",
    "This notebook experiments with re-implementing the model's source code inserting various event handlers for extension to n-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from utils import idx_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Model\n",
    "\n",
    "The model below serves as a new base model for NTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from tensorflow.python.data.experimental.ops import distribute_options\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "\n",
    "from tensorflow.python.eager import backprop\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.profiler import traceme\n",
    "\n",
    "from tensorflow.python.distribute import distribution_strategy_context as ds_context\n",
    "from tensorflow.python.keras import callbacks as callbacks_module\n",
    "from tensorflow.python.keras.utils import version_utils\n",
    "from tensorflow.python.keras.engine import training_utils\n",
    "from tensorflow.python.keras.engine import data_adapter\n",
    "from tensorflow.python.keras.engine import training\n",
    "from tensorflow.python.util import nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/data_adapter.py\n",
    "try:\n",
    "    import pandas as pd  # pylint: disable=g-import-not-at-top\n",
    "except ImportError:\n",
    "    pd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended from https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/data_adapter.py\n",
    "class WindowedDataHandler(data_adapter.DataHandler):\n",
    "    \"\"\"\n",
    "    Enumerating over this data handler yields windows of the dataset.\n",
    "    This is important for n-task because if a context switch occurs\n",
    "    during an epoch, the data needs to be sent back through the network.\n",
    "    \"\"\"\n",
    "    def enumerate_epochs(self):\n",
    "        # Calculate the number of samples for the window size\n",
    "        batch_size = self._adapter.batch_size()\n",
    "        num_samples = self._inferred_steps*batch_size\n",
    "        if self._adapter.has_partial_batch():\n",
    "            num_samples -= batch_size - self._adapter.partial_batch_size()\n",
    "        window_size = np.ceil(num_samples/min(batch_size, num_samples))\n",
    "        print(window_size)\n",
    "        # Split the dataset into windows\n",
    "        data_iterator = iter(self._dataset.window(window_size))\n",
    "        for epoch in range(self._initial_epoch, self._epochs):\n",
    "            if self._insufficient_data:\n",
    "                break\n",
    "            if self._adapter.should_recreate_iterator():\n",
    "                data_iterator = iter(self._dataset)\n",
    "            yield epoch, data_iterator\n",
    "            self._adapter.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended from https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/training.py\n",
    "class NTaskModelBase(Model):\n",
    "#     def train_step(self, data):\n",
    "#         data = data_adapter.expand_1d(data)\n",
    "#         x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "\n",
    "#         with backprop.GradientTape() as tape:\n",
    "#             y_pred = self(x, training=True)\n",
    "#             loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "#         training._minimize(self.distribute_strategy, tape, self.optimizer, loss,\n",
    "#               self.trainable_variables)\n",
    "\n",
    "#         self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "#         return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    \n",
    "    @training.enable_multi_worker\n",
    "    def fit(self,\n",
    "          x=None,\n",
    "          y=None,\n",
    "          batch_size=None,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          callbacks=None,\n",
    "          validation_split=0.,\n",
    "          validation_data=None,\n",
    "          shuffle=True,\n",
    "          class_weight=None,\n",
    "          sample_weight=None,\n",
    "          initial_epoch=0,\n",
    "          steps_per_epoch=None,\n",
    "          validation_steps=None,\n",
    "          validation_batch_size=None,\n",
    "          validation_freq=1,\n",
    "          max_queue_size=10,\n",
    "          workers=1,\n",
    "          use_multiprocessing=False):\n",
    "\n",
    "        training._keras_api_gauge.get_cell('fit').set(True)\n",
    "        # Legacy graph support is contained in `training_v1.Model`.\n",
    "        version_utils.disallow_legacy_graph('Model', 'fit')\n",
    "        self._assert_compile_was_called()\n",
    "        self._check_call_args('fit')\n",
    "\n",
    "        if validation_split:\n",
    "            # Create the validation data using the training data. Only supported for\n",
    "            # `Tensor` and `NumPy` input.\n",
    "            (x, y, sample_weight), validation_data = (\n",
    "            data_adapter.train_validation_split((x, y, sample_weight),\n",
    "                                                validation_split=validation_split,\n",
    "                                                shuffle=False))\n",
    "\n",
    "        with self.distribute_strategy.scope(), training_utils.RespectCompiledTrainableState(self):\n",
    "            # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n",
    "            data_handler = WindowedDataHandler(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                sample_weight=sample_weight,\n",
    "                batch_size=batch_size,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                initial_epoch=initial_epoch,\n",
    "                epochs=epochs,\n",
    "                shuffle=shuffle,\n",
    "                class_weight=class_weight,\n",
    "                max_queue_size=max_queue_size,\n",
    "                workers=workers,\n",
    "                use_multiprocessing=use_multiprocessing,\n",
    "                model=self)\n",
    "\n",
    "            # Container that configures and calls `tf.keras.Callback`s.\n",
    "            if not isinstance(callbacks, callbacks_module.CallbackList):\n",
    "                callbacks = callbacks_module.CallbackList(\n",
    "                    callbacks,\n",
    "                    add_history=True,\n",
    "                    add_progbar=verbose != 0,\n",
    "                    model=self,\n",
    "                    verbose=verbose,\n",
    "                    epochs=epochs,\n",
    "                    steps=data_handler.inferred_steps)\n",
    "\n",
    "            self.stop_training = False\n",
    "            train_function = self.make_train_function()\n",
    "            callbacks.on_train_begin()\n",
    "            # Handle fault-tolerance for multi-worker.\n",
    "            # TODO(omalleyt): Fix the ordering issues that mean this has to\n",
    "            # happen after `callbacks.on_train_begin`.\n",
    "            data_handler._initial_epoch = (self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n",
    "            for epoch, window_iterator in data_handler.enumerate_epochs():\n",
    "                self.reset_metrics()\n",
    "                callbacks.on_epoch_begin(epoch)\n",
    "                dataset = tf.data.Dataset.zip(next(window_iterator))\n",
    "                switched = True\n",
    "                while switched:\n",
    "                    switched = False\n",
    "                    iterator = iter(dataset)\n",
    "                    with data_handler.catch_stop_iteration():\n",
    "                        for step in data_handler.steps():\n",
    "                            with traceme.TraceMe( 'TraceContext', graph_type='train', epoch_num=epoch, step_num=step, batch_size=batch_size):\n",
    "                                callbacks.on_train_batch_begin(step)\n",
    "                                tmp_logs = train_function(iterator)\n",
    "                                # Catch OutOfRangeError for Datasets of unknown size.\n",
    "                                # This blocks until the batch has finished executing.\n",
    "                                # TODO(b/150292341): Allow multiple async steps here.\n",
    "                                if not data_handler.inferred_steps:\n",
    "                                    context.async_wait()\n",
    "                                logs = tmp_logs  # No error, now safe to assign to logs.\n",
    "                                callbacks.on_train_batch_end(step, logs)\n",
    "                epoch_logs = copy.copy(logs)\n",
    "\n",
    "                # Run validation.\n",
    "                if validation_data and self._should_eval(epoch, validation_freq):\n",
    "                    val_x, val_y, val_sample_weight = (\n",
    "                        data_adapter.unpack_x_y_sample_weight(validation_data))\n",
    "                    val_logs = self.evaluate(\n",
    "                        x=val_x,\n",
    "                        y=val_y,\n",
    "                        sample_weight=val_sample_weight,\n",
    "                        batch_size=validation_batch_size or batch_size,\n",
    "                        steps=validation_steps,\n",
    "                        callbacks=callbacks,\n",
    "                        max_queue_size=max_queue_size,\n",
    "                        workers=workers,\n",
    "                        use_multiprocessing=use_multiprocessing,\n",
    "                        return_dict=True)\n",
    "                    val_logs = {'val_' + name: val for name, val in val_logs.items()}\n",
    "                    epoch_logs.update(val_logs)\n",
    "\n",
    "                callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "                if self.stop_training:\n",
    "                    break\n",
    "\n",
    "            callbacks.on_train_end()\n",
    "            return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTaskModel(NTaskModelBase):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ModelClass, x_train, y_train, seed=5, **kwargs):\n",
    "    # Set the random seed for all used libraries\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Create the model\n",
    "    inp = Input(x_train[0].shape)\n",
    "    x = Dense(128, activation=\"relu\")(inp)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = ModelClass(inputs=inp, outputs=x)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.SGD(1e-4)\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, **kwargs)\n",
    "    \n",
    "    # Calculate and display the accuracy\n",
    "    result = (np.round(model(x_train)).astype(int).flatten() == y_train.flatten()).sum()\n",
    "    print(f\"{result}/{len(y_train)}; Accuracy: {100*result/len(y_train):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training images\n",
    "training_images = idx_load(\"../datasets/mnist/train-images.idx3-ubyte\")\n",
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training labels\n",
    "training_labels = idx_load(\"../datasets/mnist/train-labels.idx1-ubyte\")\n",
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the datasets\n",
    "training_images = training_images.reshape(len(training_images), 28*28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_gate_labels = np.array([\n",
    "    [[0], [1], [1], [0]], # XOR\n",
    "    [[1], [0], [0], [1]], # XNOR\n",
    "    [[0], [0], [0], [1]], # AND\n",
    "    [[0], [1], [1], [1]], # OR\n",
    "    [[1], [0], [0], [0]], # NOR\n",
    "    [[1], [1], [1], [0]], # NAND\n",
    "    [[1], [0], [1], [0]], # Custom 1\n",
    "    [[0], [1], [0], [1]]  # Custom 2\n",
    "])\n",
    "\n",
    "logic_gate_inputs = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST number is even\n",
    "x_train = training_images\n",
    "y_train = np.array([int(i % 2 == 0) for i in training_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[0 1 1 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify on the first 10 the dataset seems correct...\n",
    "print(training_labels[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6916\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6590\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6337\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6120\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5925\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5748\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5585\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5436\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5299\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5172\n",
      "48394/60000; Accuracy: 80.66%\n",
      "13.526658296585083\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "test(Model, x_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938.0\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6916\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6590\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6337\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6120\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5925\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5748\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5585\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5436\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5299\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5172\n",
      "48394/60000; Accuracy: 80.66%\n",
      "13.328534841537476\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "test(ExtendedModel, x_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 846us/step - loss: 0.7324\n",
      "2/4; Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "test(Model, logic_gate_inputs, logic_gate_labels[0], epochs=1, batch_size=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2/2 [==============================] - 0s 801us/step - loss: 0.7324\n",
      "2/4; Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "test(ExtendedModel, logic_gate_inputs, logic_gate_labels[0], epochs=1, batch_size=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = g_dataset.window(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset shapes: (1, 2), types: tf.int64> <_VariantDataset shapes: (1, 1), types: tf.int64>\n",
      "tf.Tensor([[-1  1]], shape=(1, 2), dtype=int64)\n",
      "tf.Tensor([[-1 -1]], shape=(1, 2), dtype=int64)\n",
      "tf.Tensor([[1 1]], shape=(1, 2), dtype=int64)\n",
      "tf.Tensor([[ 1 -1]], shape=(1, 2), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for win_x, win_y in epochs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = (i for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = lambda: next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-521-4ce711c44abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_OptionsDataset shapes: ((1, 2), (1, 1)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OptionsDataset' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-527-00a2e6cef2b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_OptionsDataset' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "g_dataset.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter([1, 2, 3])\n",
    "b = iter([4, 5, 6])\n",
    "c = next(zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
