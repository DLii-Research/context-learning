{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Source\n",
    "\n",
    "This notebook experiments with re-implementing the model's source code inserting various event handlers for extension to n-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Layer\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from collections import defaultdict\n",
    "from enum import IntFlag, auto\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utils import idx_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrr(length, normalized=True):\n",
    "    \"\"\"Create a new HRR vector using Tensorflow tensors\"\"\"\n",
    "    length = int(length)      \n",
    "    shp = int((length-1)/2)\n",
    "    if normalized:    \n",
    "        x = tf.random.uniform( shape = (shp,), minval = -np.pi, maxval = np.pi, dtype = tf.dtypes.float32, seed = 100, name = None )\n",
    "        x = tf.cast(x, tf.complex64)\n",
    "        if length % 2:\n",
    "            x = tf.math.real( tf.signal.ifft( tf.concat([tf.ones(1, dtype=\"complex64\"), tf.exp(1j*x), tf.exp(-1j*x[::-1])], axis=0)))\n",
    "\n",
    "        else:  \n",
    "            x = tf.math.real(tf.signal.ifft(tf.concat([tf.ones(1, dtype=\"complex64\"),tf.exp(1j*x),tf.ones(1, dtype=\"complex64\"),tf.exp(-1j*x[::-1])],axis=0)))\n",
    "    else:        \n",
    "        x = tf.random.normal( shape = (length,), mean=0.0, stddev=1.0/tf.sqrt(float(length)),dtype=tf.dtypes.float32,seed=100,name=None)\n",
    "    return x\n",
    "\n",
    "\n",
    "def hrrs(length, n=1, normalized=True):\n",
    "    \"\"\"Create n new HRR vectors using Tensorflow tensors\"\"\"\n",
    "    return tf.stack([hrr(length, normalized) for x in range(n)], axis=0)\n",
    "\n",
    "\n",
    "def circ_conv(x, y):\n",
    "    \"\"\"Calculate the circular convolution between two HRR vectors\"\"\"\n",
    "    x = tf.cast(x, tf.complex64)\n",
    "    y = tf.cast(y, tf.complex64)\n",
    "    return tf.math.real(tf.signal.ifft(tf.signal.fft(x)*tf.signal.fft(y)))\n",
    "\n",
    "\n",
    "def logmod(x):\n",
    "    return np.sign(x)*np.log(abs(x) + 1)\n",
    "    \n",
    "    \n",
    "def plot(title, labels, *frameGroups):\n",
    "    fig, ax = plt.subplots()\n",
    "    plotFrames(ax, title, labels, *frameGroups, xlabel=\"Epoch\", ylabel=\"Value\")\n",
    "    ax.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def plotFrames(ax, title, labels, *frameGroups, xlabel=None, ylabel=None):\n",
    "    for i, frames in enumerate(frameGroups):\n",
    "        keys = tuple(frames.keys() if type(frames) == dict else range(len(frames)))\n",
    "        t = np.arange(keys[0], keys[-1] + 1, 1)\n",
    "        ax.plot(t, list(frames.values()), label=(labels[i] if labels else None))\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Verbosity(IntFlag):\n",
    "    Progress = auto()\n",
    "    Contexts = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrModel:\n",
    "    \n",
    "    def __init__(self, switch_threshold, add_threshold=0.0, max_contexts=0):\n",
    "        \n",
    "        self.switch_threshold = tf.Variable(switch_threshold, name=\"Switch_Threshold\", trainable=False, dtype=tf.float32)\n",
    "        self.add_threshold = tf.Variable(add_threshold or 0.0, name=\"Add_Threshold\", trainable=False, dtype=tf.float32)\n",
    "        self._max_contexts = max_contexts\n",
    "        self._context_layer = None\n",
    "        \n",
    "        # Track the number of sequential switches so we can determine if no tasks fit the threshold\n",
    "        self._num_seq_switches = tf.Variable(0, name=\"Sequential_Switches\", trainable=False, dtype=tf.int64)\n",
    "        \n",
    "        # Indicate the epoch at which the last switch occurred\n",
    "        self.epoch_switched = tf.Variable(-1, name=\"Last_Switch_Epoch\", trainable=False, dtype=tf.int64)\n",
    "        \n",
    "        # If we tried to add another context after the max number of contexts was reached, we should warn the user\n",
    "        # and stop checking whether or not a context should be added\n",
    "        self._exceeded_context_limit = tf.Variable(False, name=\"Context_Limit_Exceeded\", trainable=False, dtype=tf.bool)\n",
    "        \n",
    "        # Track the context-loss delta for the active context\n",
    "        self.delta = tf.Variable(0.0, name=\"ATR_Delta\", trainable=False, dtype=tf.float32)\n",
    "        \n",
    "        # Store the delta that triggered the initial context switch\n",
    "        self.delta_switched = tf.Variable(0.0, name=\"ATR_Delta_Switched\", trainable=False, dtype=tf.float32)\n",
    "        \n",
    "        # To be built...\n",
    "        self.values = None\n",
    "        self.values_initialized = None\n",
    "        \n",
    "        \n",
    "    def set_context_layer(self, context_layer):\n",
    "        self._context_layer = context_layer\n",
    "    \n",
    "    \n",
    "    def build(self, num_contexts):\n",
    "        # Determine the number of contexts to create.\n",
    "        # Since we can't yet dynamically add contexts, we need\n",
    "        # to create the list at its max size initially.\n",
    "        num_contexts = max(num_contexts, self._max_contexts)\n",
    "        \n",
    "        # Create the list of ATR values to track\n",
    "        self.values = tf.Variable(np.zeros(num_contexts), name=\"ATR_Values\", trainable=False, dtype=tf.float32)\n",
    "        \n",
    "        # A second list is created to determine uninitialized ATR values\n",
    "        self.values_initialized = tf.Variable(np.zeros(num_contexts), name=\"ATR_Values_Initialized\", trainable=False, dtype=tf.bool)\n",
    "        \n",
    "    \n",
    "    def add_context(self):\n",
    "        # Add the new context to the context layer\n",
    "        self._context_layer.add_context()\n",
    "        \n",
    "        \n",
    "    def switch_contexts(self, context_loss, verbose):\n",
    "        \n",
    "        # If we have exhausted the context list, look for the one with the best fit\n",
    "        if self._num_seq_switches >= self.num_contexts:\n",
    "            best_fit = self.find_best_fit_context()\n",
    "            \n",
    "            # If no context really fits well and we can add more contexts, add a new one\n",
    "            if self.max_num_contexts > 0 and not self._exceeded_context_limit and self.should_add_context(context_loss, best_fit):\n",
    "                if self.num_contexts < self.max_num_contexts:\n",
    "                    self.add_context()\n",
    "                    self.hot_context = self.num_contexts - 1\n",
    "                    if verbose & Verbosity.Contexts:\n",
    "                        tf.print(f\"\\n[{self.context_layer.name}] Adding context {self.hot_context}\")\n",
    "                else:\n",
    "                    self._exceeded_context_limit.assign(True)\n",
    "                    tf.print(f\"\\n[{self.context_layer.name}] WARNING: Attempted to add context after context limit reached\")\n",
    "                \n",
    "            # Use the best fit context\n",
    "            else:\n",
    "                if verbose & Verbosity.Contexts:\n",
    "                    tf.print(f\"\\nUsing best-fit context {best_fit}\")\n",
    "                # Switch to the best-fitting context\n",
    "                self.hot_context = best_fit\n",
    "                \n",
    "                # Before the ATR value is updated...\n",
    "#                 self.on_before_update(context_loss)\n",
    "                \n",
    "                # Update the ATR value for the new context\n",
    "                self.update_atr_value(self.context_losses[self.hot_context], switched=True)\n",
    "\n",
    "        else:\n",
    "            self.context_layer.next_context()\n",
    "                \n",
    "    \n",
    "    def update_and_switch(self, epoch, context_loss, dynamic_switch, verbose):\n",
    "        \"\"\"\n",
    "        Update the ATR.\n",
    "        \n",
    "        Returns result type\n",
    "        \"\"\"\n",
    "        if dynamic_switch and self.should_switch(epoch, context_loss):\n",
    "            \n",
    "            # Before we switch...\n",
    "            self.on_before_switch(epoch, context_loss)\n",
    "            \n",
    "            # Count the switches\n",
    "            self._num_seq_switches.assign_add(1)\n",
    "            self.epoch_switched.assign(epoch)\n",
    "            \n",
    "            # Switch contexts and return the result\n",
    "            self.switch_contexts(context_loss, verbose)\n",
    "            \n",
    "            # Switched, so nothing was updated\n",
    "            return False\n",
    "        \n",
    "        # Before the ATR value is updated...\n",
    "        self.on_before_update(context_loss)\n",
    "            \n",
    "        self.update_atr_value(context_loss, switched=False)\n",
    "            \n",
    "        # Reset the switch count if we previously switched\n",
    "        if self._num_seq_switches != 0:\n",
    "            self._num_seq_switches.assign(0)\n",
    "            if verbose & Verbosity.Contexts:\n",
    "                tf.print(f\"\\n[{self.context_layer.name}] Switched context to {self.hot_context}\")\n",
    "            \n",
    "        # Updated successfully\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def set_atr_value(self, context_loss):\n",
    "        self.values.scatter_nd_update([[self.hot_context]], [context_loss])\n",
    "        if not self.values_initialized[self.hot_context]:\n",
    "            self.values_initialized.scatter_nd_update([[self.hot_context]], [True])\n",
    "\n",
    "    # Event Handlers ------------------------------------------------------------------------------\n",
    "    \n",
    "    def on_before_switch(self, epoch, context_loss):\n",
    "        if epoch != self.epoch_switched:\n",
    "            delta = self.values[self.hot_context] - context_loss\n",
    "            self.delta_switched.assign(delta)\n",
    "            self.delta.assign(delta)\n",
    "            \n",
    "    def on_before_update(self, context_loss):\n",
    "        if self.values_initialized[self.hot_context]:\n",
    "            delta = self.values[self.hot_context] - context_loss\n",
    "            self.delta.assign(delta)\n",
    "            \n",
    "    # Overridable ---------------------------------------------------------------------------------\n",
    "    \n",
    "    def context_loss_fn(self, context_delta):\n",
    "        # Calculate Context Error\n",
    "        # Keras MSE must have both args be arrs of floats, if one or both are arrs of ints, the output will be rounded to an int\n",
    "        # This is how responsible the context layer was for the loss\n",
    "        return tf.keras.losses.mean_squared_error(np.zeros(len(context_delta)), context_delta)\n",
    "    \n",
    "    def update_atr_value(self, context_loss, switched):\n",
    "        \"\"\"Update the ATR value\"\"\"\n",
    "        # Update the ATR value\n",
    "        self.set_atr_value(context_loss)\n",
    "    \n",
    "    def find_best_fit_context(self):\n",
    "        \"\"\"Locate the context index with the best fit\"\"\"\n",
    "        return tf.argmax(tf.subtract(self.values, self.context_losses)[:self.num_contexts])\n",
    "    \n",
    "    def should_switch(self, epoch, context_loss):\n",
    "        # If the ATR value has not been initialized yet, we don't need to switch\n",
    "        if not self.values_initialized[self.hot_context]:\n",
    "            return False\n",
    "        # If the context loss exceeds the threshold\n",
    "        delta = self.values[self.hot_context] - context_loss\n",
    "        return delta < self.switch_threshold\n",
    "    \n",
    "    def should_add_context(self, context_loss, best_fit_context_idx):\n",
    "        \"\"\"\n",
    "        Determine if a new context should be added\n",
    "        Note: This is only checked after a switch has been determined\n",
    "        \"\"\"\n",
    "        delta = self.values[self.hot_context] - self.context_losses[best_fit_context_idx]\n",
    "        return delta < self.add_threshold\n",
    "    \n",
    "    def epoch_traces(self, epoch):\n",
    "        \"\"\"\n",
    "        Return a dictionary of traces to plot\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"ATR Traces\": [\n",
    "                trace(f\"Context {i}\", v) for i, v in enumerate(self.values.value())\n",
    "                      if self.values_initialized[i] is not None\n",
    "            ],\n",
    "            \"Delta Trace\": [\n",
    "                trace(\"Switch Threshold\", self.switch_threshold.value(), '--', 'grey'), # Dark grey is lighter than grey...\n",
    "                trace(\"Add Threshold\", self.add_threshold.value(), '-.', 'grey', condition=self.max_num_contexts>0),\n",
    "                trace(\"Context Delta\", self.delta_switched.value(), '-', condition=self.epoch_switched==epoch),\n",
    "                trace(\"Context Delta\", self.delta.value(), '-')\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    # Properties ----------------------------------------------------------------------------------\n",
    "        \n",
    "    @property\n",
    "    def context_losses(self):\n",
    "        return self.context_layer.context_losses\n",
    "        \n",
    "    @property\n",
    "    def num_contexts(self):\n",
    "        if self.context_layer is None:\n",
    "            return None\n",
    "        return self.context_layer.num_contexts\n",
    "        \n",
    "    @property\n",
    "    def max_num_contexts(self):\n",
    "        return self._max_contexts\n",
    "    \n",
    "    @property\n",
    "    def context_layer(self):\n",
    "        return self._context_layer\n",
    "    \n",
    "    @property\n",
    "    def hot_context(self):\n",
    "        return self._context_layer.hot_context\n",
    "    \n",
    "    @hot_context.setter\n",
    "    def hot_context(self, hot_context):\n",
    "        self._context_layer.hot_context = hot_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrMovingAverage(AtrModel):\n",
    "    def update_atr_value(self, context_loss, switched):\n",
    "        if switched or not self.values_initialized[self.hot_context]:\n",
    "            self.set_atr_value(context_loss)\n",
    "        else:\n",
    "            self.set_atr_value((self.values[self.hot_context] + context_loss) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context(Layer):\n",
    "    \n",
    "    def __init__(self, contexts=1, atr_model=None, **kwargs):\n",
    "        super(Context, self).__init__(**kwargs)\n",
    "        \n",
    "        # The ATR model handles the switching mechanisms\n",
    "        self._atr_model = atr_model\n",
    "        \n",
    "        # Information Tracking\n",
    "#         self._context_loss = tf.Variable([0.0, 0.0], name=\"Context_Losses\", trainable=False, dtype=float) # Created in build step\n",
    "        self._num_contexts = tf.Variable(contexts, name=\"Num_Contexts\", trainable=False, dtype=tf.int64)\n",
    "        self._hot_context = tf.Variable(0, name=\"Hot_Context\", trainable=False, dtype=tf.int64)\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Store the input shape since weights can be rebuilt later\n",
    "        self._input_shape = int(input_shape[-1])\n",
    "        \n",
    "        # Build the ATR model\n",
    "        self._atr_model.set_context_layer(self)\n",
    "        self._atr_model.build(self.num_contexts)\n",
    "        \n",
    "        # The number of contexts to create in the kernel\n",
    "        num_kernel_contexts = max(self.num_contexts, self.atr_model.max_num_contexts)\n",
    "        \n",
    "        # Create the HRR initializer. This will create the list of HRR vectors\n",
    "        initializer = lambda shape, dtype=None: hrrs(self._input_shape, n=num_kernel_contexts)\n",
    "        self.kernel = self.add_weight(name=\"context\", shape=[num_kernel_contexts, self._input_shape], initializer=initializer, trainable=False)\n",
    "        \n",
    "        # Store the context losses for each context\n",
    "        self._context_loss = tf.Variable(np.zeros(num_kernel_contexts), name=\"Context_Losses\", trainable=False, dtype=float)\n",
    "        \n",
    "        #TEMP\n",
    "        self._max_contexts = num_kernel_contexts\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculate the output for this layer.\n",
    "        \n",
    "        This layer convolves the input values with the context HRR vector\n",
    "        to produce the output tensor.\n",
    "        \"\"\"\n",
    "        # Fetch the hot context's HRR vector\n",
    "        context_hrr = self.kernel[self.hot_context]\n",
    "        \n",
    "        # Return the resulting convolution between the inputs and the context HRR\n",
    "        return circ_conv(inputs, context_hrr)\n",
    "    \n",
    "    \n",
    "    def update_and_switch(self, epoch, dynamic_switch, verbose):\n",
    "        \"\"\"\n",
    "        Update ATR values and switch contexts if necessary.\n",
    "        Returns True if no context switch occurs; False otherwise\n",
    "        \"\"\"\n",
    "        # If there is no ATR model, there's nothing to update\n",
    "        if self._atr_model is None:\n",
    "            return True\n",
    "        \n",
    "        # Update the ATR madel\n",
    "        result = self._atr_model.update_and_switch(epoch, self.context_loss, dynamic_switch, verbose)\n",
    "        \n",
    "        # Clear the context loss when we're done\n",
    "        self.clear_context_loss()\n",
    "        \n",
    "        # Did the ATR model update or switch?\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    #TODO Context adding\n",
    "    def add_context(self):\n",
    "        # kernel_arr = self.kernel.value()\n",
    "        # num_hrrs = max(0, self._num_contexts - len(kernel_arr))\n",
    "        # initializer = lambda shape, dtype=None: np.append(kernel_arr[:self.num_contexts], hrrs(self._input_shape, n=num_hrrs), axis=0)\n",
    "        # new_weights = self.add_weight(name=\"context\", shape=[self.num_contexts, self._input_shape], initializer=initializer, trainable=False)\n",
    "        # Create the weights for the layer.\n",
    "        # The weights in this layer are generated HRR vectors, and are never updated.\n",
    "        # self.kernel = new_weights\n",
    "        \n",
    "        if self._num_contexts < self._max_contexts:\n",
    "            self._num_contexts.assign_add(1)\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    \n",
    "    def clear_context_loss(self):\n",
    "        \"\"\"Clear the context loss for the current epoch\"\"\"\n",
    "        self._context_loss.scatter_nd_update([[self.hot_context]], [0.0])\n",
    "    \n",
    "    \n",
    "    def add_context_loss(self, context_loss):\n",
    "        \"\"\"Accumulate context loss\"\"\"\n",
    "        if self._atr_model is not None:\n",
    "            context_loss = self._atr_model.context_loss_fn(context_loss)\n",
    "        else:\n",
    "            context_loss = tf.keras.losses.mean_squared_error(np.zeros(len(context_loss)), context_loss)\n",
    "        self._context_loss.scatter_nd_add([[self.hot_context]], [context_loss])\n",
    "        \n",
    "        \n",
    "    def next_context(self):\n",
    "        \"\"\"Switch to the next sequential context\"\"\"\n",
    "        self.hot_context = (self.hot_context + 1) % self.num_contexts\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def atr_model(self):\n",
    "        return self._atr_model\n",
    "    \n",
    "    @property\n",
    "    def context_loss(self):\n",
    "        return self._context_loss[self.hot_context]\n",
    "    \n",
    "    @property\n",
    "    def context_losses(self):\n",
    "        return self._context_loss\n",
    "        \n",
    "    @property\n",
    "    def num_contexts(self):\n",
    "        return self._num_contexts.value()\n",
    "    \n",
    "    @property\n",
    "    def hot_context(self):\n",
    "        \"\"\"Get the active context index\"\"\"\n",
    "        return self._hot_context.value()\n",
    "    \n",
    "    @hot_context.setter\n",
    "    def hot_context(self, hot_context):\n",
    "        if hot_context not in range(self.num_contexts):\n",
    "            raise ValueError(\"`Provided context does not exist\")\n",
    "        self._hot_context.assign(hot_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Model\n",
    "\n",
    "The model below serves as a new base model for NTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer as lso\n",
    "from tensorflow.python.data.experimental.ops import distribute_options\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "\n",
    "from tensorflow.python.eager import backprop\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.profiler import traceme\n",
    "\n",
    "from tensorflow.python.distribute import distribution_strategy_context as ds_context\n",
    "from tensorflow.python.distribute import parameter_server_strategy\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras import callbacks as callbacks_module\n",
    "from tensorflow.python.keras.utils import version_utils\n",
    "from tensorflow.python.keras.engine import training_utils\n",
    "from tensorflow.python.keras.engine import data_adapter\n",
    "from tensorflow.python.keras.engine import training\n",
    "from tensorflow.python.util import nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/data_adapter.py\n",
    "try:\n",
    "    import pandas as pd  # pylint: disable=g-import-not-at-top\n",
    "except ImportError:\n",
    "    pd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _minimize(strategy, tape, optimizer, loss, trainable_variables):\n",
    "    \"\"\"Minimizes loss for one step by updating `trainable_variables`.\n",
    "    This is roughly equivalent to\n",
    "    ```python\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    ```\n",
    "    However, this function also applies gradient clipping and loss scaling if the\n",
    "    optimizer is a LossScaleOptimizer.\n",
    "    Args:\n",
    "      strategy: `tf.distribute.Strategy`.\n",
    "      tape: A gradient tape. The loss must have been computed under this tape.\n",
    "      optimizer: The optimizer used to minimize the loss.\n",
    "      loss: The loss tensor.\n",
    "      trainable_variables: The variables that will be updated in order to minimize\n",
    "        the loss.\n",
    "    Return:\n",
    "      gradients\n",
    "    \"\"\"\n",
    "\n",
    "    with tape:\n",
    "        if isinstance(optimizer, lso.LossScaleOptimizer):\n",
    "            loss = optimizer.get_scaled_loss(loss)\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    # Whether to aggregate gradients outside of optimizer. This requires support\n",
    "    # of the optimizer and doesn't work with ParameterServerStrategy and\n",
    "    # CentralStroageStrategy.\n",
    "    aggregate_grads_outside_optimizer = (\n",
    "        optimizer._HAS_AGGREGATE_GRAD and  # pylint: disable=protected-access\n",
    "        not isinstance(strategy.extended,\n",
    "                       parameter_server_strategy.ParameterServerStrategyExtended))\n",
    "\n",
    "    if aggregate_grads_outside_optimizer:\n",
    "        # We aggregate gradients before unscaling them, in case a subclass of\n",
    "        # LossScaleOptimizer all-reduces in fp16. All-reducing in fp16 can only be\n",
    "        # done on scaled gradients, not unscaled gradients, for numeric stability.\n",
    "        gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access\n",
    "                                                       trainable_variables))\n",
    "    if isinstance(optimizer, lso.LossScaleOptimizer):\n",
    "        gradients = optimizer.get_unscaled_gradients(gradients)\n",
    "    gradients = optimizer._clip_gradients(gradients)  # pylint: disable=protected-access\n",
    "    if trainable_variables:\n",
    "        if aggregate_grads_outside_optimizer:\n",
    "            optimizer.apply_gradients(\n",
    "                zip(gradients, trainable_variables),\n",
    "                experimental_aggregate_gradients=False)\n",
    "        else:\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended from https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/data_adapter.py\n",
    "class WindowedDataHandler(data_adapter.DataHandler):\n",
    "    \"\"\"\n",
    "    Enumerating over this data handler yields windows of the dataset.\n",
    "    This is important for n-task because if a context switch occurs\n",
    "    during an epoch the data needs to be sent back through the network.\n",
    "    \"\"\"\n",
    "    def calc_window_size(self):\n",
    "        batch_size = self._adapter.batch_size()\n",
    "        num_samples = self._inferred_steps*batch_size\n",
    "        if self._adapter.has_partial_batch():\n",
    "            num_samples -= batch_size - self._adapter.partial_batch_size()\n",
    "        return np.ceil(num_samples/min(batch_size, num_samples))\n",
    "    \n",
    "    def enumerate_epochs(self):\n",
    "        data_iterator = iter(self._dataset.window(self.calc_window_size()))\n",
    "        for epoch in range(self._initial_epoch, self._epochs):\n",
    "            if self._insufficient_data:\n",
    "                break\n",
    "            if self._adapter.should_recreate_iterator():\n",
    "                data_iterator = iter(self._dataset.window(self.calc_window_size()))\n",
    "            yield epoch, data_iterator\n",
    "            self._adapter.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended from https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/training.py\n",
    "class NTaskModelBase(Model):\n",
    "    \"\"\"\n",
    "    This abstract model integrates the raw mechanisms and handlers into\n",
    "    Tensorflow Keras' model class. These mechanisms can be implemented by\n",
    "    inheriting from this class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NTaskModelBase, self).__init__(*args, **kwargs)\n",
    "        self.accumulate_gradients = False\n",
    "        self.accumulated_gradients = None\n",
    "        \n",
    "        \n",
    "    def compile(self, *args, accumulate_gradients=False, **kwargs):\n",
    "        super(NTaskModelBase, self).compile(*args, **kwargs)\n",
    "        \n",
    "        # TODO\n",
    "        if accumulate_gradients:\n",
    "            self.accumulate_gradients = True\n",
    "        \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        data = data_adapter.expand_1d(data)\n",
    "        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "\n",
    "        with backprop.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "            \n",
    "        gradients = _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n",
    "              self.trainable_variables)\n",
    "        \n",
    "        # Add context loss to layers\n",
    "        self.add_context_loss(gradients)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    \n",
    "    @training.enable_multi_worker\n",
    "    def fit(self,\n",
    "            x=None,\n",
    "            y=None,\n",
    "            batch_size=None,\n",
    "            epochs=1,\n",
    "            verbose=1,\n",
    "            dynamic_switch=True,\n",
    "            callbacks=None,\n",
    "            validation_split=0.,\n",
    "            validation_data=None,\n",
    "            shuffle=True,\n",
    "            class_weight=None,\n",
    "            sample_weight=None,\n",
    "            initial_epoch=0,\n",
    "            steps_per_epoch=None,\n",
    "            validation_steps=None,\n",
    "            validation_batch_size=None,\n",
    "            validation_freq=1,\n",
    "            max_queue_size=10,\n",
    "            workers=1,\n",
    "            use_multiprocessing=False):\n",
    "\n",
    "        training._keras_api_gauge.get_cell('fit').set(True)\n",
    "        # Legacy graph support is contained in `training_v1.Model`.\n",
    "        version_utils.disallow_legacy_graph('Model', 'fit')\n",
    "        self._assert_compile_was_called()\n",
    "        self._check_call_args('fit')\n",
    "\n",
    "        if validation_split:\n",
    "            # Create the validation data using the training data. Only supported for\n",
    "            # `Tensor` and `NumPy` input.\n",
    "            (x, y, sample_weight), validation_data = (\n",
    "            data_adapter.train_validation_split((x, y, sample_weight),\n",
    "                                                validation_split=validation_split,\n",
    "                                                shuffle=False))\n",
    "\n",
    "        with self.distribute_strategy.scope(), training_utils.RespectCompiledTrainableState(self):\n",
    "            # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n",
    "            data_handler = WindowedDataHandler(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                sample_weight=sample_weight,\n",
    "                batch_size=batch_size,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                initial_epoch=initial_epoch,\n",
    "                epochs=epochs,\n",
    "                shuffle=shuffle,\n",
    "                class_weight=class_weight,\n",
    "                max_queue_size=max_queue_size,\n",
    "                workers=workers,\n",
    "                use_multiprocessing=use_multiprocessing,\n",
    "                model=self)\n",
    "\n",
    "            # Container that configures and calls `tf.keras.Callback`s.\n",
    "            if not isinstance(callbacks, callbacks_module.CallbackList):\n",
    "                callbacks = callbacks_module.CallbackList(\n",
    "                    callbacks,\n",
    "                    add_history=True,\n",
    "                    add_progbar=bool(verbose & Verbosity.Progress),\n",
    "                    model=self,\n",
    "                    verbose=verbose,\n",
    "                    epochs=epochs,\n",
    "                    steps=data_handler.inferred_steps)\n",
    "\n",
    "            self.stop_training = False\n",
    "            train_function = self.make_train_function()\n",
    "            callbacks.on_train_begin()\n",
    "            # Handle fault-tolerance for multi-worker.\n",
    "            # TODO(omalleyt): Fix the ordering issues that mean this has to\n",
    "            # happen after `callbacks.on_train_begin`.\n",
    "            data_handler._initial_epoch = (self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n",
    "            for epoch, window_iterator in data_handler.enumerate_epochs():\n",
    "                self.reset_metrics()\n",
    "                callbacks.on_epoch_begin(epoch)\n",
    "                dataset = tf.data.Dataset.zip(next(window_iterator))\n",
    "                switched = True\n",
    "                weights = backend.batch_get_value(self.trainable_variables)\n",
    "                while switched:\n",
    "                    self.initialize_epoch(epoch)\n",
    "                    iterator = iter(dataset)\n",
    "                    with data_handler.catch_stop_iteration():\n",
    "                        for step in data_handler.steps():\n",
    "                            with traceme.TraceMe( 'TraceContext', graph_type='train', epoch_num=epoch, step_num=step, batch_size=batch_size):\n",
    "                                callbacks.on_train_batch_begin(step)\n",
    "                                tmp_logs = train_function(iterator)\n",
    "                                # Catch OutOfRangeError for Datasets of unknown size.\n",
    "                                # This blocks until the batch has finished executing.\n",
    "                                # TODO(b/150292341): Allow multiple async steps here.\n",
    "                                if not data_handler.inferred_steps:\n",
    "                                    context.async_wait()\n",
    "                                logs = tmp_logs  # No error, now safe to assign to logs.\n",
    "                                callbacks.on_train_batch_end(step, logs)\n",
    "                                \n",
    "                        switched = not self.update_and_switch(epoch, dynamic_switch, verbose)\n",
    "                        # If a switch occurred, we need to restore the weights\n",
    "                        if switched:\n",
    "                            backend.batch_set_value(zip(self.trainable_variables, weights))\n",
    "                            self.reset_metrics()\n",
    "                    \n",
    "                epoch_logs = copy.copy(logs)\n",
    "                \n",
    "                if self.accumulate_gradients:\n",
    "                    self.optimizer.apply_gradients(zip(self.accumulated_gradients, self.trainable_variables))\n",
    "\n",
    "                # Run validation.\n",
    "                if validation_data and self._should_eval(epoch, validation_freq):\n",
    "                    val_x, val_y, val_sample_weight = (\n",
    "                        data_adapter.unpack_x_y_sample_weight(validation_data))\n",
    "                    val_logs = self.evaluate(\n",
    "                        x=val_x,\n",
    "                        y=val_y,\n",
    "                        sample_weight=val_sample_weight,\n",
    "                        batch_size=validation_batch_size or batch_size,\n",
    "                        steps=validation_steps,\n",
    "                        callbacks=callbacks,\n",
    "                        max_queue_size=max_queue_size,\n",
    "                        workers=workers,\n",
    "                        use_multiprocessing=use_multiprocessing,\n",
    "                        return_dict=True)\n",
    "                    val_logs = {'val_' + name: val for name, val in val_logs.items()}\n",
    "                    epoch_logs.update(val_logs)\n",
    "\n",
    "                callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "                if self.stop_training:\n",
    "                    break\n",
    "\n",
    "            callbacks.on_train_end()\n",
    "            return self.history\n",
    "        \n",
    "    def add_context_loss(self, gradients):\n",
    "        \"\"\"Calculate and add context loss to context layers\"\"\"\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def initialize_epoch(self, epoch):\n",
    "        \"\"\"Reset context loss in context layers\"\"\"\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def update_and_switch(self, epoch, dynamic_switch=True, verbose=0):\n",
    "        \"\"\"\n",
    "        Update the context layers\n",
    "        \n",
    "        Args:\n",
    "            dynamic_switch [bool]: Enable/disable dynamic switching mechanisms\n",
    "        Return:\n",
    "            [bool]: Indicate if no switches occurred\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTaskModel(NTaskModelBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NTaskModel, self).__init__(*args, **kwargs)\n",
    "        self.ctx_layers = [i for i, layer in enumerate(self.layers) if isinstance(layer, Context)]\n",
    "        \n",
    "        # We need to map the context layer to their gradient indices\n",
    "        self.ctx_gradient_map = {}\n",
    "        index = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Context):\n",
    "                self.ctx_gradient_map[i] = index + 1 # The bias gradient\n",
    "            index += len(layer.trainable_variables)\n",
    "    \n",
    "    \n",
    "    def _calc_context_loss(self, ctx_layer_idx, gradients):\n",
    "        \"\"\"\n",
    "        IMPORTANT: \n",
    "        1) Assumes no use of activation function on Ntask layer\n",
    "        2) Assumes that the layer following the Ntask layer:\n",
    "            a) Is a Dense layer\n",
    "            b) Is using bias\n",
    "               — ex: Dense(20, ... , use_bias=True) \n",
    "               — note Keras Dense layer uses bias by default if no value is given for use_bias param\n",
    "        3) Assumes index of the next layer's gradient is known within the gradients list returned from gradient tape in a tape.gradient call\n",
    "        4) If the above points aren't met, things will break and it may be hard to locate the bugs\n",
    "        \"\"\"\n",
    "        # From the delta rule in neural network math        \n",
    "        index = self.ctx_gradient_map[ctx_layer_idx]\n",
    "        delta_at_next_layer = gradients[index]\n",
    "        transpose_of_weights_at_next_layer = tf.transpose(self.layers[ctx_layer_idx + 1].weights[0])\n",
    "        \n",
    "        # Calculate delta at n-task layer\n",
    "        context_delta = tf.tensordot(delta_at_next_layer, transpose_of_weights_at_next_layer, 1)\n",
    "        return context_delta\n",
    "    \n",
    "    \n",
    "    def initialize_epoch(self, epoch):\n",
    "        # Clear context loss (probably going to use a new mechanism here)\n",
    "#         for i in self.ctx_layers:\n",
    "#             self.layers[i].clear_context_loss()\n",
    "        pass\n",
    "            \n",
    "    \n",
    "    def add_context_loss(self, gradients):\n",
    "        for i in self.ctx_layers:\n",
    "            self.layers[i].add_context_loss(self._calc_context_loss(i, gradients))\n",
    "    \n",
    "    \n",
    "    def update_and_switch(self, epoch, dynamic_switch, verbose):\n",
    "        updated = True\n",
    "        for i in reversed(self.ctx_layers):\n",
    "            layer = self.layers[i]\n",
    "            updated &= layer.update_and_switch(epoch, dynamic_switch=dynamic_switch, verbose=verbose)\n",
    "        return updated\n",
    "    \n",
    "    \n",
    "    def get_contexts(self):\n",
    "        return [self.layers[layer].hot_context for layer in self.ctx_layers]\n",
    "    \n",
    "\n",
    "    def set_contexts(self, contexts):\n",
    "        for i, layer in enumerate(self.ctx_layers):\n",
    "            self.layers[layer].hot_context = contexts[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrLoggerTensorBoard(tf.keras.callbacks.BaseLogger):\n",
    "    \"\"\"\n",
    "    Log ATR models via TensorBoard\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, logdir, *args, **kwargs):\n",
    "        super(AtrLoggerTensorBoard, self).__init__(*args, **kwargs)\n",
    "        self.logdir = logdir\n",
    "        self.writers = {}\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        super(AtrLoggerTensorBoard, self).set_model(model)\n",
    "        self.writers = {self.model.layers[i]: [] for i in self.model.ctx_layers}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Create the correct number of writers for the task if necessary\"\"\"\n",
    "        for layer, writers in self.writers.items():\n",
    "            for i in range(len(writers), layer.num_contexts):\n",
    "                writers.append(tf.summary.create_file_writer(os.path.join(self.logdir, f\"{layer.name}_Atr_{i}\")))\n",
    "            plot_tag = f\"{layer.name}_Atr_Trace\"\n",
    "            for i, writer in enumerate(writers):\n",
    "                with writer.as_default():\n",
    "                    value = layer.atr_model.values[i]\n",
    "                    if value is not None:\n",
    "                        tf.summary.scalar(plot_tag, data=value, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrLogger(tf.keras.callbacks.BaseLogger):\n",
    "    \"\"\"\n",
    "    Log ATR models via matplotlib\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AtrLogger, self).__init__(*args, **kwargs)\n",
    "        self.plots = None # layer_name -> { plot_name -> { trace_name -> { data } } }\n",
    "        self.model = None\n",
    "        \n",
    "    def plot(self, vertical=True, figsize=(30, 6)):\n",
    "        for layer, plots in self.plots.items():\n",
    "            dim = (len(plots), 1) if vertical else (1, len(plots))\n",
    "            fig, axs = plt.subplots(*dim, figsize=figsize, sharey=False)\n",
    "            for i, (plot_name, traces) in enumerate(plots.items()):\n",
    "                for label, trace in traces.items():\n",
    "                    axs[i].plot(\n",
    "                        trace[\"x\"], trace[\"y\"],\n",
    "                        label=label,\n",
    "                        color=trace[\"color\"],\n",
    "                        linestyle=trace[\"style\"])\n",
    "                axs[i].set_title(plot_name)\n",
    "                axs[i].set_xlabel(\"Epoch\")\n",
    "                axs[i].set_ylabel(\"Value\")\n",
    "                axs[i].grid(True)\n",
    "                axs[i].legend()\n",
    "            fig.suptitle(layer.name)\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        if not isinstance(model, NTaskModel):\n",
    "            return\n",
    "        if self.model != model:\n",
    "            self.plots = {}\n",
    "            for i in model.ctx_layers:\n",
    "                layer = model.layers[i]\n",
    "                if layer.atr_model is not None:\n",
    "                    self.plots[layer] = defaultdict(lambda: defaultdict(lambda: {\n",
    "                        \"x\": [],\n",
    "                        \"y\": [],\n",
    "                        \"style\": None,\n",
    "                        \"color\": None\n",
    "                    }))\n",
    "        super(AtrLogger, self).set_model(model)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for layer, plots in self.plots.items():\n",
    "            for plot_name, traces in layer.atr_model.epoch_traces(epoch).items():\n",
    "                for trace_data in traces:\n",
    "                    if trace_data is not None:\n",
    "                        (label, value, style, color) = trace_data\n",
    "                        trace = plots[plot_name][label]\n",
    "                        if len(trace[\"x\"]) == 0:\n",
    "                            trace[\"style\"] = style\n",
    "                            trace[\"color\"] = color\n",
    "                        trace[\"x\"].append(epoch)\n",
    "                        trace[\"y\"].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(label, value, style='-', color=None, condition=True):\n",
    "    if not condition:\n",
    "        return None\n",
    "    return (label, value, style, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          x_train,\n",
    "          y_train_list,\n",
    "          cycles=1,\n",
    "          epochs=1,\n",
    "          task_shuffle=True,\n",
    "          initial_task_shuffle=False,\n",
    "          explicit_contexts=None,\n",
    "          assert_contexts=False,\n",
    "          **kwargs):\n",
    "    \"\"\"\n",
    "    Train an NTask model on a dataset containing samples from multiple contexts.\n",
    "    \n",
    "    Args:\n",
    "      model            : The NTask model instance\n",
    "      x_train          : The input data\n",
    "      y_train_list     : A list of tuples containing the y_train value and context IDs.\n",
    "                         e.g. (y_train, 0, 2) # (y_train, context_layer_0 = 0, context_layer_1 = 2)\n",
    "      cycles           : The number of iterations over the entire y_train_list\n",
    "      epochs           : The number of epochs to perform on each y_train element within y_train_list\n",
    "      task_shuffle     : Shuffle the y_train_list on each cycle (does not shuffle data within the task)\n",
    "      initial_shuffle  : Shuffle the y_train_list on the first epoch. If set to false, context order\n",
    "                         within the context layers is guaranteed to remain in the same order as provided\n",
    "      explicit_contexts: A list of context mappings for each task. If none (either entirely, or\n",
    "                         contains None as an item), the current task will be dynamically switched.\n",
    "      **kwargs         : All other keyword arguments are passed to `model.fit`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate provided explicit contexts\n",
    "    if explicit_contexts is None:\n",
    "        explicit_contexts = [None]*len(y_train_list)\n",
    "    elif len(explicit_contexts) != len(y_train_list):\n",
    "        raise ValueError(f\"Supplied number of explicit contexts ({len(explicit_contexts)}) does not match the number of tasks ({len(y_train_list)}).\")\n",
    "    else:\n",
    "        for i, task in enumerate(explicit_contexts):\n",
    "            if task is not None and len(task) != len(model.context_layers):\n",
    "                raise ValueError(f\"Provided explicit contexts for task {i} does not match the number of context layers.\")\n",
    "    \n",
    "    # Create an index map of the tasks\n",
    "    indices = np.arange(len(y_train_list))\n",
    "    \n",
    "    # Shuffle if necessary\n",
    "    if initial_task_shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    # Map the shuffled tasks in the order they are passed to the `fit` method\n",
    "    task_map = indices.copy()\n",
    "    \n",
    "    # Track the layer contexts for each task for later evaluation\n",
    "    context_map = [None]*len(indices)\n",
    "    \n",
    "    for cycle in range(cycles):\n",
    "        for i, task in enumerate(indices):\n",
    "            y_train = y_train_list[task]\n",
    "            \n",
    "            # Calculate the initial epoch to start training on\n",
    "            initial_epoch = cycle*len(y_train_list)*epochs + i*epochs\n",
    "            end_epoch = initial_epoch + epochs\n",
    "            \n",
    "            # Set contexts explicitly if necessary\n",
    "            if explicit_contexts[task] is not None:\n",
    "                model.set_contexts(explicit_contexts[task])\n",
    "                \n",
    "            model.fit(x_train, y_train, epochs=end_epoch, initial_epoch=initial_epoch, dynamic_switch=(explicit_contexts[task] is None), **kwargs)\n",
    "            \n",
    "            # Update the task map\n",
    "            context_map[task] = model.get_contexts()\n",
    "                \n",
    "        if task_shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "    return task_map, context_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,\n",
    "             x, y_list,\n",
    "             task_map,\n",
    "             context_map,\n",
    "             display_predictions=True,\n",
    "             return_dict=True,\n",
    "             **kwargs):\n",
    "    results = []\n",
    "#     reverse_lookup = np.zeros(len(task_map))\n",
    "#     for i, task in enumerate(task_map):\n",
    "#         reverse_lookup[task] = i\n",
    "    for i, task in enumerate(task_map):\n",
    "        y = y_list[i]\n",
    "        contexts = context_map[i]\n",
    "        \n",
    "        model.set_contexts(contexts)\n",
    "        if display_predictions:\n",
    "            tf.print(model.predict(x))\n",
    "        results.append(model.evaluate(x, y, return_dict=return_dict, **kwargs))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ModelClass, init_args, compile_args, x_train, y_train, seed=5, **kwargs):\n",
    "    # Set the random seed for all used libraries\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Create the model\n",
    "    inp = Input(x_train[0].shape)\n",
    "    x = Dense(128, activation=\"relu\")(inp)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = ModelClass(inputs=inp, outputs=x, **init_args)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.SGD(1e-4),\n",
    "        **compile_args\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, **kwargs)\n",
    "    \n",
    "    # Calculate and display the accuracy\n",
    "    result = (np.round(model(x_train)).astype(int).flatten() == y_train.flatten()).sum()\n",
    "    print(f\"{result}/{len(y_train)}; Accuracy: {100*result/len(y_train):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_context(ModelClass, init_args, compile_args, x_train, y_train_list, cycles=1, seed=5, epochs=1, **kwargs):\n",
    "    # Set the random seed for all used libraries\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Create the model\n",
    "    inp = Input(x_train[0].shape)\n",
    "    x = Dense(128, activation=\"relu\", use_bias=True)(inp)\n",
    "    x = Context(2, AtrMovingAverage(max_contexts=2, switch_threshold=-0.02))(x)\n",
    "#     x = Context()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\", use_bias=True)(x)\n",
    "    model = ModelClass(inputs=inp, outputs=x, **init_args)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.SGD(1e-1),\n",
    "        **compile_args\n",
    "    )\n",
    "    \n",
    "#     logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#     callbacks = [\n",
    "#         tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n",
    "#         AtrLoggerTensorBoard(logdir)\n",
    "#     ]\n",
    "    \n",
    "    callbacks = [\n",
    "        AtrLogger()\n",
    "    ]\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    for cycle in range(cycles):\n",
    "        for context, y_train in enumerate(y_train_list):\n",
    "            initial_epoch = cycle*len(y_train_list)*epochs + context*epochs\n",
    "#             model.set_contexts([context])\n",
    "            model.fit(x_train, y_train, callbacks=callbacks, initial_epoch=initial_epoch, epochs=initial_epoch + epochs, **kwargs)\n",
    "    \n",
    "    for context in range(len(y_train_list)):\n",
    "        model.set_contexts([context])\n",
    "        tf.print(model.predict(x_train))\n",
    "    \n",
    "    # Calculate and display the accuracy\n",
    "    result = (np.round(model(x_train)).astype(int).flatten() == y_train.flatten()).sum()\n",
    "    print(f\"{result}/{len(y_train)}; Accuracy: {100*result/len(y_train):.2f}%\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_context_dynamic(ModelClass, init_args, compile_args, x_train, y_train_list, cycles=1, seed=5, epochs=1, **kwargs):\n",
    "    # Set the random seed for all used libraries\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Create the model\n",
    "    inp = Input(x_train[0].shape)\n",
    "    x = Dense(128, activation=\"relu\", use_bias=True)(inp)\n",
    "    x = Context(1, AtrMovingAverage(max_contexts=len(y_train_list)+1, switch_threshold=-0.02, add_threshold=-0.04))(x)\n",
    "    x = Dense(1, activation=\"sigmoid\", use_bias=True)(x)\n",
    "    model = ModelClass(inputs=inp, outputs=x, **init_args)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.SGD(1e-1),\n",
    "        **compile_args\n",
    "    )\n",
    "    \n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n",
    "        AtrLogger(logdir)\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    for cycle in range(cycles):\n",
    "        for context, y_train in enumerate(y_train_list):\n",
    "            initial_epoch = cycle*len(y_train_list)*epochs + context*epochs\n",
    "#             model.set_contexts([context])\n",
    "            model.fit(x_train, y_train, callbacks=callbacks, initial_epoch=initial_epoch, epochs=initial_epoch + epochs, **kwargs)\n",
    "    \n",
    "    for context in range(len(y_train_list)):\n",
    "        model.set_contexts([context])\n",
    "        tf.print(model.predict(x_train))\n",
    "    \n",
    "    # Calculate and display the accuracy\n",
    "    result = (np.round(model(x_train)).astype(int).flatten() == y_train.flatten()).sum()\n",
    "    print(f\"{result}/{len(y_train)}; Accuracy: {100*result/len(y_train):.2f}%\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training images\n",
    "training_images = idx_load(\"../datasets/mnist/train-images.idx3-ubyte\")\n",
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training labels\n",
    "training_labels = idx_load(\"../datasets/mnist/train-labels.idx1-ubyte\")\n",
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the datasets\n",
    "training_images = training_images.reshape(len(training_images), 28*28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_gate_labels = np.array([\n",
    "    [[0], [1], [1], [0]], # XOR\n",
    "    [[1], [0], [0], [1]], # XNOR\n",
    "    [[0], [0], [0], [1]], # AND\n",
    "    [[0], [1], [1], [1]], # OR\n",
    "    [[1], [0], [0], [0]], # NOR\n",
    "    [[1], [1], [1], [0]], # NAND\n",
    "    [[1], [0], [1], [0]], # Custom 1\n",
    "    [[0], [1], [0], [1]]  # Custom 2\n",
    "])\n",
    "\n",
    "logic_gate_inputs = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST number is even\n",
    "x_train = training_images\n",
    "y_train = np.array([int(i % 2 == 0) for i in training_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[0 1 1 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify on the first 10 the dataset seems correct...\n",
    "print(training_labels[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6916 - accuracy: 0.5445\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6590 - accuracy: 0.6339\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6337 - accuracy: 0.6905\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6120 - accuracy: 0.7269\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5925 - accuracy: 0.7527\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5748 - accuracy: 0.7689\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5585 - accuracy: 0.7812\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5436 - accuracy: 0.7921\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7985\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5172 - accuracy: 0.8041\n",
      "48394/60000; Accuracy: 80.66%\n",
      "CPU times: user 32.4 s, sys: 18 s, total: 50.4 s\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%time test(Model, {}, {\"metrics\": [\"accuracy\"]}, x_train, y_train, epochs=10, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6916 - accuracy: 0.5445\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6590 - accuracy: 0.6339\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6337 - accuracy: 0.6905\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6120 - accuracy: 0.7269\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5925 - accuracy: 0.7527\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5748 - accuracy: 0.7689\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5585 - accuracy: 0.7812\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5436 - accuracy: 0.7921\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7985\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5172 - accuracy: 0.8041\n",
      "48394/60000; Accuracy: 80.66%\n",
      "CPU times: user 31.9 s, sys: 17.2 s, total: 49.1 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%time test(NTaskModel, {}, {\"metrics\": [\"accuracy\"]}, x_train, y_train, epochs=10, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/4; Accuracy: 50.00%\n",
      "CPU times: user 3.06 s, sys: 406 ms, total: 3.47 s\n",
      "Wall time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "%time test(NTaskModel, {}, {}, logic_gate_inputs, logic_gate_labels[0], epochs=500, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictWrapper({})\n",
      "1/1 [==============================] - 0s 870us/step - loss: 0.7324\n",
      "2/4; Accuracy: 50.00%\n",
      "CPU times: user 250 ms, sys: 15.6 ms, total: 266 ms\n",
      "Wall time: 243 ms\n"
     ]
    }
   ],
   "source": [
    "%time test(NTaskModel, {}, {}, logic_gate_inputs, logic_gate_labels[0], epochs=1, batch_size=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_contexts([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 893us/step - loss: 6.0130e-04 - binary_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0006013047532178462, 1.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(logic_gate_inputs, logic_gate_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_context' is not defined"
     ]
    }
   ],
   "source": [
    "%time model = test_context(NTaskModel, {}, {\"metrics\": [tf.keras.metrics.BinaryAccuracy()]}, logic_gate_inputs, logic_gate_labels[:2], cycles=3, epochs=10, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([\n",
    "    [[0], [1], [1], [0]], # XOR\n",
    "    [[1], [0], [0], [1]], # XNOR\n",
    "    [[0], [0], [0], [1]], # AND\n",
    "    [[0], [1], [1], [1]], # OR\n",
    "    [[1], [0], [0], [0]], # NOR\n",
    "    [[1], [1], [1], [0]], # NAND\n",
    "    [[1], [0], [1], [0]], # Custom 1\n",
    "    [[0], [1], [0], [1]]  # Custom 2\n",
    "])\n",
    "\n",
    "x_train = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_gate_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 4469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4164d8399f767c45\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4164d8399f767c45\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
