{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Source\n",
    "\n",
    "This notebook experiments with re-implementing the model's source code inserting various event handlers for extension to n-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Layer\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utils import idx_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_metric(metric):\n",
    "    \"\"\"Copy a new metric instance from an existing instance\"\"\"\n",
    "    return tf.keras.metrics.deserialize(tf.keras.metrics.serialize(metric))\n",
    "\n",
    "\n",
    "def hrr(length, normalized=True):\n",
    "    \"\"\"Create a new HRR vector using Tensorflow tensors\"\"\"\n",
    "    length = int(length)      \n",
    "    shp = int((length-1)/2)\n",
    "    if normalized:    \n",
    "        x = tf.random.uniform( shape = (shp,), minval = -np.pi, maxval = np.pi, dtype = tf.dtypes.float32, seed = 100, name = None )\n",
    "        x = tf.cast(x, tf.complex64)\n",
    "        if length % 2:\n",
    "            x = tf.math.real( tf.signal.ifft( tf.concat([tf.ones(1, dtype=\"complex64\"), tf.exp(1j*x), tf.exp(-1j*x[::-1])], axis=0)))\n",
    "\n",
    "        else:  \n",
    "            x = tf.math.real(tf.signal.ifft(tf.concat([tf.ones(1, dtype=\"complex64\"),tf.exp(1j*x),tf.ones(1, dtype=\"complex64\"),tf.exp(-1j*x[::-1])],axis=0)))\n",
    "    else:        \n",
    "        x = tf.random.normal( shape = (length,), mean=0.0, stddev=1.0/tf.sqrt(float(length)),dtype=tf.dtypes.float32,seed=100,name=None)\n",
    "    return x\n",
    "\n",
    "\n",
    "def hrrs(length, n=1, normalized=True):\n",
    "    \"\"\"Create n new HRR vectors using Tensorflow tensors\"\"\"\n",
    "    return tf.stack([hrr(length, normalized) for x in range(n)], axis=0)\n",
    "\n",
    "\n",
    "def circ_conv(x, y):\n",
    "    \"\"\"Calculate the circular convolution between two HRR vectors\"\"\"\n",
    "    x = tf.cast(x, tf.complex64)\n",
    "    y = tf.cast(y, tf.complex64)\n",
    "    return tf.math.real(tf.signal.ifft(tf.signal.fft(x)*tf.signal.fft(y)))\n",
    "\n",
    "\n",
    "def logmod(x):\n",
    "    return np.sign(x)*np.log(abs(x) + 1)\n",
    "    \n",
    "    \n",
    "def plot(title, labels, *frameGroups):\n",
    "    fig, ax = plt.subplots()\n",
    "    plotFrames(ax, title, labels, *frameGroups, xlabel=\"Epoch\", ylabel=\"Value\")\n",
    "    ax.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def plotFrames(ax, title, labels, *frameGroups, xlabel=None, ylabel=None):\n",
    "    for i, frames in enumerate(frameGroups):\n",
    "        keys = tuple(frames.keys() if type(frames) == dict else range(len(frames)))\n",
    "        t = np.arange(keys[0], keys[-1] + 1, 1)\n",
    "        ax.plot(t, list(frames.values()), label=(labels[i] if labels else None))\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context(Layer):\n",
    "    \n",
    "    RESULT_UPDATED  = 0 # The ATR model was updated successfully\n",
    "    RESULT_SWITCHED = 1 # A task switch was triggered in the ATR\n",
    "    RESULT_ADDED    = 2 # A new task was added to the ATR model\n",
    "    \n",
    "    def __init__(self, num_tasks=1):\n",
    "        super(Context, self).__init__()\n",
    "        \n",
    "        self.kernel = None\n",
    "        \n",
    "        # Information Tracking\n",
    "        self.num_tasks = num_tasks\n",
    "        self._hot_context = tf.Variable(0, name=\"Hot_Context\", trainable=False, dtype=tf.int32)\n",
    "        self.context_loss = tf.Variable(0.0, name=\"Context_Loss\", trainable=False, dtype=float)\n",
    "        \n",
    "        \n",
    "    def _setup_hrr_weights(self):\n",
    "        # Create the HRR initializer. This will create the list of HRR vectors\n",
    "        if self.kernel is None:\n",
    "            initializer = lambda shape, dtype=None: hrrs(self._input_shape, n=self.num_tasks)\n",
    "        else:\n",
    "            # If there are previously generated HRRs, they should be retained\n",
    "            kernel_arr = self.kernel.numpy()\n",
    "            num_hrrs = max(0, self.num_tasks - len(kernel_arr))\n",
    "            initializer = lambda shape, dtype=None: np.append(kernel_arr[:self.num_tasks], hrrs(self._input_shape, n=num_hrrs), axis=0)\n",
    "        \n",
    "        # Create the weights for the layer.\n",
    "        # The weights in this layer are generated HRR vectors, and are never updated.\n",
    "        self.kernel = self.add_weight(name=\"context\", shape=[self.num_tasks, self._input_shape], initializer=initializer, trainable=False)\n",
    "        \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Store the input shape since weights can be rebuilt later\n",
    "        self._input_shape = int(input_shape[-1])\n",
    "        \n",
    "        # Build the n-task information\n",
    "        self._setup_hrr_weights()\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculate the output for this layer.\n",
    "        \n",
    "        This layer convolves the input values with the context HRR vector\n",
    "        to produce the output tensor.\n",
    "        \"\"\"\n",
    "        tf.print(\"Executing on task:\", self.hot_context)\n",
    "        \n",
    "        # Fetch the hot context's HRR vector\n",
    "        context_hrr = self.kernel[self.hot_context]\n",
    "        \n",
    "        # Return the resulting convolution between the inputs and the context HRR\n",
    "        return circ_conv(inputs, context_hrr)\n",
    "    \n",
    "    \n",
    "    def update_and_switch(self, dynamic_switch=True, verbose=1):\n",
    "        \"\"\"\n",
    "        Update ATR values and switch contexts if necessary.\n",
    "        Returns True if no context switch occurs; False otherwise\n",
    "        \"\"\"\n",
    "        # Update the ATR values. If a task switch occurs, check if a task was added...\n",
    "        \n",
    "        # No task switched occurred, updated successfully\n",
    "        return Context.RESULT_UPDATED\n",
    "        \n",
    "    \n",
    "    def clear_context_loss(self):\n",
    "        \"\"\"Clear the context loss for the current epoch\"\"\"\n",
    "        self.context_loss.assign(0.0)\n",
    "    \n",
    "    \n",
    "    def add_context_loss(self, context_delta):\n",
    "        \"\"\"Accumulate context loss\"\"\"\n",
    "        context_loss = tf.keras.losses.mean_squared_error(np.zeros(len(context_delta)), context_delta)\n",
    "        self.context_loss.assign_add(context_loss)\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def hot_context(self):\n",
    "        \"\"\"Get the active context index\"\"\"\n",
    "        return self._hot_context.value()\n",
    "    \n",
    "    \n",
    "    @hot_context.setter\n",
    "    def hot_context(self, hot_context):\n",
    "        self._hot_context.assign(hot_context)\n",
    "        self.context_loss.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Model\n",
    "\n",
    "The model below serves as a new base model for NTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer as lso\n",
    "from tensorflow.python.data.experimental.ops import distribute_options\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "\n",
    "from tensorflow.python.eager import backprop\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.profiler import traceme\n",
    "\n",
    "from tensorflow.python.distribute import distribution_strategy_context as ds_context\n",
    "from tensorflow.python.distribute import parameter_server_strategy\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras import callbacks as callbacks_module\n",
    "from tensorflow.python.keras.utils import version_utils\n",
    "from tensorflow.python.keras.engine import training_utils\n",
    "from tensorflow.python.keras.engine import data_adapter\n",
    "from tensorflow.python.keras.engine import training\n",
    "from tensorflow.python.util import nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/data_adapter.py\n",
    "try:\n",
    "    import pandas as pd  # pylint: disable=g-import-not-at-top\n",
    "except ImportError:\n",
    "    pd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _minimize(strategy, tape, optimizer, loss, trainable_variables):\n",
    "    \"\"\"Minimizes loss for one step by updating `trainable_variables`.\n",
    "    This is roughly equivalent to\n",
    "    ```python\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    ```\n",
    "    However, this function also applies gradient clipping and loss scaling if the\n",
    "    optimizer is a LossScaleOptimizer.\n",
    "    Args:\n",
    "      strategy: `tf.distribute.Strategy`.\n",
    "      tape: A gradient tape. The loss must have been computed under this tape.\n",
    "      optimizer: The optimizer used to minimize the loss.\n",
    "      loss: The loss tensor.\n",
    "      trainable_variables: The variables that will be updated in order to minimize\n",
    "        the loss.\n",
    "    Return:\n",
    "      gradients\n",
    "    \"\"\"\n",
    "\n",
    "    with tape:\n",
    "        if isinstance(optimizer, lso.LossScaleOptimizer):\n",
    "            loss = optimizer.get_scaled_loss(loss)\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    # Whether to aggregate gradients outside of optimizer. This requires support\n",
    "    # of the optimizer and doesn't work with ParameterServerStrategy and\n",
    "    # CentralStroageStrategy.\n",
    "    aggregate_grads_outside_optimizer = (\n",
    "        optimizer._HAS_AGGREGATE_GRAD and  # pylint: disable=protected-access\n",
    "        not isinstance(strategy.extended,\n",
    "                       parameter_server_strategy.ParameterServerStrategyExtended))\n",
    "\n",
    "    if aggregate_grads_outside_optimizer:\n",
    "        # We aggregate gradients before unscaling them, in case a subclass of\n",
    "        # LossScaleOptimizer all-reduces in fp16. All-reducing in fp16 can only be\n",
    "        # done on scaled gradients, not unscaled gradients, for numeric stability.\n",
    "        gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access\n",
    "                                                       trainable_variables))\n",
    "    if isinstance(optimizer, lso.LossScaleOptimizer):\n",
    "        gradients = optimizer.get_unscaled_gradients(gradients)\n",
    "    gradients = optimizer._clip_gradients(gradients)  # pylint: disable=protected-access\n",
    "    if trainable_variables:\n",
    "        if aggregate_grads_outside_optimizer:\n",
    "            optimizer.apply_gradients(\n",
    "                zip(gradients, trainable_variables),\n",
    "                experimental_aggregate_gradients=False)\n",
    "        else:\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended from https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/data_adapter.py\n",
    "class WindowedDataHandler(data_adapter.DataHandler):\n",
    "    \"\"\"\n",
    "    Enumerating over this data handler yields windows of the dataset.\n",
    "    This is important for n-task because if a context switch occurs\n",
    "    during an epoch the data needs to be sent back through the network.\n",
    "    \"\"\"\n",
    "    def calc_window_size(self):\n",
    "        batch_size = self._adapter.batch_size()\n",
    "        num_samples = self._inferred_steps*batch_size\n",
    "        if self._adapter.has_partial_batch():\n",
    "            num_samples -= batch_size - self._adapter.partial_batch_size()\n",
    "        return np.ceil(num_samples/min(batch_size, num_samples))\n",
    "    \n",
    "    def enumerate_epochs(self):\n",
    "        data_iterator = iter(self._dataset.window(self.calc_window_size()))\n",
    "        for epoch in range(self._initial_epoch, self._epochs):\n",
    "            if self._insufficient_data:\n",
    "                break\n",
    "            if self._adapter.should_recreate_iterator():\n",
    "                data_iterator = iter(self._dataset.window(self.calc_window_size()))\n",
    "            yield epoch, data_iterator\n",
    "            self._adapter.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended from https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/training.py\n",
    "class NTaskModelBase(Model):\n",
    "    \"\"\"\n",
    "    This abstract model integrates the raw mechanisms and handlers into\n",
    "    Tensorflow Keras' model class. These mechanisms can be implemented by\n",
    "    inheriting from this class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NTaskModelBase, self).__init__(*args, **kwargs)\n",
    "        self.accumulate_gradients = False\n",
    "        self.accumulated_gradients = None\n",
    "        \n",
    "        \n",
    "    def compile(self, *args, accumulate_gradients=False, **kwargs):\n",
    "        super(NTaskModelBase, self).compile(*args, **kwargs)\n",
    "        \n",
    "        # TODO\n",
    "        if accumulate_gradients:\n",
    "            self.accumulate_gradients = True\n",
    "        \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        data = data_adapter.expand_1d(data)\n",
    "        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "\n",
    "        with backprop.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "            \n",
    "        gradients = _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n",
    "              self.trainable_variables)\n",
    "        \n",
    "        # Add context loss to layers\n",
    "        self.add_context_loss(gradients)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    \n",
    "    @training.enable_multi_worker\n",
    "    def fit(self,\n",
    "            x=None,\n",
    "            y=None,\n",
    "            batch_size=None,\n",
    "            epochs=1,\n",
    "            verbose=1,\n",
    "            dynamic_switch=True,\n",
    "            callbacks=None,\n",
    "            validation_split=0.,\n",
    "            validation_data=None,\n",
    "            shuffle=True,\n",
    "            class_weight=None,\n",
    "            sample_weight=None,\n",
    "            initial_epoch=0,\n",
    "            steps_per_epoch=None,\n",
    "            validation_steps=None,\n",
    "            validation_batch_size=None,\n",
    "            validation_freq=1,\n",
    "            max_queue_size=10,\n",
    "            workers=1,\n",
    "            use_multiprocessing=False):\n",
    "        \n",
    "        tf.print(\"Hot contexts:\", [layer.hot_context for layer in self.layers if isinstance(layer, Context)])\n",
    "\n",
    "        training._keras_api_gauge.get_cell('fit').set(True)\n",
    "        # Legacy graph support is contained in `training_v1.Model`.\n",
    "        version_utils.disallow_legacy_graph('Model', 'fit')\n",
    "        self._assert_compile_was_called()\n",
    "        self._check_call_args('fit')\n",
    "\n",
    "        if validation_split:\n",
    "            # Create the validation data using the training data. Only supported for\n",
    "            # `Tensor` and `NumPy` input.\n",
    "            (x, y, sample_weight), validation_data = (\n",
    "            data_adapter.train_validation_split((x, y, sample_weight),\n",
    "                                                validation_split=validation_split,\n",
    "                                                shuffle=False))\n",
    "\n",
    "        with self.distribute_strategy.scope(), training_utils.RespectCompiledTrainableState(self):\n",
    "            # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n",
    "            data_handler = WindowedDataHandler(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                sample_weight=sample_weight,\n",
    "                batch_size=batch_size,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                initial_epoch=initial_epoch,\n",
    "                epochs=epochs,\n",
    "                shuffle=shuffle,\n",
    "                class_weight=class_weight,\n",
    "                max_queue_size=max_queue_size,\n",
    "                workers=workers,\n",
    "                use_multiprocessing=use_multiprocessing,\n",
    "                model=self)\n",
    "\n",
    "            # Container that configures and calls `tf.keras.Callback`s.\n",
    "            if not isinstance(callbacks, callbacks_module.CallbackList):\n",
    "                callbacks = callbacks_module.CallbackList(\n",
    "                    callbacks,\n",
    "                    add_history=True,\n",
    "                    add_progbar=verbose != 0,\n",
    "                    model=self,\n",
    "                    verbose=verbose,\n",
    "                    epochs=epochs,\n",
    "                    steps=data_handler.inferred_steps)\n",
    "\n",
    "            self.stop_training = False\n",
    "            train_function = self.make_train_function()\n",
    "            callbacks.on_train_begin()\n",
    "            # Handle fault-tolerance for multi-worker.\n",
    "            # TODO(omalleyt): Fix the ordering issues that mean this has to\n",
    "            # happen after `callbacks.on_train_begin`.\n",
    "            data_handler._initial_epoch = (self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n",
    "            for epoch, window_iterator in data_handler.enumerate_epochs():\n",
    "                self.reset_metrics()\n",
    "                callbacks.on_epoch_begin(epoch)\n",
    "                dataset = tf.data.Dataset.zip(next(window_iterator))\n",
    "                switched = True\n",
    "                weights = backend.batch_get_value(self.trainable_variables)\n",
    "                while switched:\n",
    "                    self.initialize_epoch(epoch)\n",
    "                    iterator = iter(dataset)\n",
    "                    with data_handler.catch_stop_iteration():\n",
    "                        for step in data_handler.steps():\n",
    "                            with traceme.TraceMe( 'TraceContext', graph_type='train', epoch_num=epoch, step_num=step, batch_size=batch_size):\n",
    "                                callbacks.on_train_batch_begin(step)\n",
    "                                tmp_logs = train_function(iterator)\n",
    "                                # Catch OutOfRangeError for Datasets of unknown size.\n",
    "                                # This blocks until the batch has finished executing.\n",
    "                                # TODO(b/150292341): Allow multiple async steps here.\n",
    "                                if not data_handler.inferred_steps:\n",
    "                                    context.async_wait()\n",
    "                                logs = tmp_logs  # No error, now safe to assign to logs.\n",
    "                                callbacks.on_train_batch_end(step, logs)\n",
    "                        switched = not self.update_and_switch(dynamic_switch, verbose)\n",
    "                        # If a switch occurred, we need to restore the weights\n",
    "                        if switched:\n",
    "                            backend.batch_set_value(zip(self.trainable_variables, weights))\n",
    "                            self.reset_metrics()\n",
    "                    \n",
    "                epoch_logs = copy.copy(logs)\n",
    "                \n",
    "                if self.accumulate_gradients:\n",
    "                    self.optimizer.apply_gradients(zip(self.accumulated_gradients, self.trainable_variables))\n",
    "\n",
    "                # Run validation.\n",
    "                if validation_data and self._should_eval(epoch, validation_freq):\n",
    "                    val_x, val_y, val_sample_weight = (\n",
    "                        data_adapter.unpack_x_y_sample_weight(validation_data))\n",
    "                    val_logs = self.evaluate(\n",
    "                        x=val_x,\n",
    "                        y=val_y,\n",
    "                        sample_weight=val_sample_weight,\n",
    "                        batch_size=validation_batch_size or batch_size,\n",
    "                        steps=validation_steps,\n",
    "                        callbacks=callbacks,\n",
    "                        max_queue_size=max_queue_size,\n",
    "                        workers=workers,\n",
    "                        use_multiprocessing=use_multiprocessing,\n",
    "                        return_dict=True)\n",
    "                    val_logs = {'val_' + name: val for name, val in val_logs.items()}\n",
    "                    epoch_logs.update(val_logs)\n",
    "\n",
    "                callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "                if self.stop_training:\n",
    "                    break\n",
    "\n",
    "            callbacks.on_train_end()\n",
    "            return self.history\n",
    "        \n",
    "    def add_context_loss(self, gradients):\n",
    "        \"\"\"Calculate and add context loss to context layers\"\"\"\n",
    "        return\n",
    "        raise NotImplemented(\"`add_context_loss` not implemented\")\n",
    "        \n",
    "        \n",
    "    def initialize_epoch(self, epoch):\n",
    "        \"\"\"Reset context loss in context layers\"\"\"\n",
    "        return\n",
    "        raise NotImplemented(\"`initialize_epoch` not implemneted\")\n",
    "        \n",
    "        \n",
    "    def update_and_switch(self, dynamic_switch=True, verbose=0):\n",
    "        \"\"\"\n",
    "        Update the context layers\n",
    "        \n",
    "        Args:\n",
    "            dynamic_switch [bool]: Enable/disable dynamic switching mechanisms\n",
    "        Return:\n",
    "            [bool]: Indicate if no switches occurred\n",
    "        \"\"\"\n",
    "        return True\n",
    "        raise NotImplemented(\"`update_and_switch` not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTaskModel(NTaskModelBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NTaskModel, self).__init__(*args, **kwargs)\n",
    "        self.ctx_layers = [i for i, layer in enumerate(self.layers) if isinstance(layer, Context)]\n",
    "        \n",
    "        # We need to map the context layer to their gradient indices\n",
    "        self.ctx_gradient_map = {}\n",
    "        index = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Context):\n",
    "                self.ctx_gradient_map[i] = index + 1 # The bias gradient\n",
    "            index += len(layer.trainable_variables)\n",
    "    \n",
    "    \n",
    "    def _calc_context_loss(self, ctx_layer_idx, gradients):\n",
    "        \"\"\"\n",
    "        IMPORTANT: \n",
    "        1) Assumes no use of activation function on Ntask layer\n",
    "        2) Assumes that the layer following the Ntask layer:\n",
    "            a) Is a Dense layer\n",
    "            b) Is using bias \n",
    "               — ex: Dense(20, ... , use_bias=True) \n",
    "               — note Keras Dense layer uses bias by default if no value is given for use_bias param\n",
    "        3) Assumes index of the next layer's gradient is known within the gradients list returned from gradient tape in a tape.gradient call\n",
    "        4) If the above points aren't met, things will break and it may be hard to locate the bugs\n",
    "        \"\"\"\n",
    "        # From the delta rule in neural network math\n",
    "        index = self.ctx_gradient_map[ctx_layer_idx]\n",
    "        delta_at_next_layer = gradients[index]\n",
    "        transpose_of_weights_at_next_layer = tf.transpose(self.layers[ctx_layer_idx + 1].weights[0])\n",
    "#         transpose_of_weights_at_next_layer = tf.transpose(self.layers[index + 1].get_weights()[0])\n",
    "        \n",
    "        # Calculate delta at n-task layer\n",
    "#         context_delta = np.dot(delta_at_next_layer, transpose_of_weights_at_next_layer).astype(np.float)\n",
    "        context_delta = tf.tensordot(delta_at_next_layer, transpose_of_weights_at_next_layer, 1)\n",
    "        return context_delta\n",
    "    \n",
    "    \n",
    "    def initialize_epoch(self, epoch):            \n",
    "        # Clear context loss (probably going to use a new mechanism here)\n",
    "        for i in self.ctx_layers:\n",
    "            self.layers[i].clear_context_loss()\n",
    "            \n",
    "    \n",
    "    def add_context_loss(self, gradients):\n",
    "        for i in self.ctx_layers:\n",
    "            self.layers[i].add_context_loss(self._calc_context_loss(i, gradients))\n",
    "    \n",
    "    \n",
    "    def update_and_switch(self, dynamic_switch, verbose):\n",
    "        switched = False\n",
    "        for i in reversed(self.ctx_layers):\n",
    "            layer = self.layers[i]\n",
    "            result = layer.update_and_switch(dynamic_switch=dynamic_switch, verbose=verbose)\n",
    "            if result & Context.RESULT_SWITCHED:\n",
    "                switched = True\n",
    "        return not switched\n",
    "    \n",
    "\n",
    "    def set_contexts(self, contexts):\n",
    "        for i, layer in enumerate(self.ctx_layers):\n",
    "            self.layers[layer].hot_context = contexts[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrLogger(tf.keras.callbacks.BaseLogger):\n",
    "    \n",
    "    def __init__(self, logdir, *args, **kwargs):\n",
    "        super(AtrLogger, self).__init__(*args, **kwargs)\n",
    "        self.logdir = logdir\n",
    "        self.writers = {}\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        super(AtrLogger, self).set_model(model)\n",
    "        self.writers = {self.model.layers[i]: [] for i in self.model.ctx_layers}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Create the correct number of writers for the task if necessary\"\"\"\n",
    "        for layer, writers in self.writers.items():\n",
    "            for i in range(len(writers), layer.num_tasks):\n",
    "                writers.append(tf.summary.create_file_writer(os.path.join(self.logdir, f\"context_atr_{i}\"))) # TODO Fix this name here...\n",
    "            plot_tag = f\"context_atr_trace\"                                                                  # TODO Fix this name here too...\n",
    "            for i, writer in enumerate(writers):\n",
    "                with writer.as_default():\n",
    "                    value = layer.atr_model.atr_values[i]\n",
    "                    if value is not None:\n",
    "                        tf.summary.scalar(plot_tag, data=value, step=epoch)\n",
    "    \n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         self._update_writers()\n",
    "#         v = np.random.random()\n",
    "#         for i, writer in enumerate(self.writers):\n",
    "#             with writer.as_default():\n",
    "#                 tf.summary.scalar(\"atr_traces\", data=v+i, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ModelClass, init_args, compile_args, x_train, y_train, seed=5, **kwargs):\n",
    "    # Set the random seed for all used libraries\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Create the model\n",
    "    inp = Input(x_train[0].shape)\n",
    "    x = Dense(128, activation=\"relu\")(inp)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = ModelClass(inputs=inp, outputs=x, **init_args)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.SGD(1e-4),\n",
    "        **compile_args\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, **kwargs)\n",
    "    \n",
    "    # Calculate and display the accuracy\n",
    "    result = (np.round(model(x_train)).astype(int).flatten() == y_train.flatten()).sum()\n",
    "    print(f\"{result}/{len(y_train)}; Accuracy: {100*result/len(y_train):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_context(ModelClass, init_args, compile_args, x_train, y_train_list, cycles=1, seed=5, epochs=1, **kwargs):\n",
    "    # Set the random seed for all used libraries\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Create the model\n",
    "    inp = Input(x_train[0].shape)\n",
    "    x = Dense(128, activation=\"relu\", use_bias=True)(inp)\n",
    "    x = Context(num_tasks=2)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\", use_bias=True)(x)\n",
    "    model = ModelClass(inputs=inp, outputs=x, **init_args)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.SGD(1e-1),\n",
    "        **compile_args\n",
    "    )\n",
    "    \n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n",
    "#         AtrLogger(logdir)\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    for cycle in range(cycles):\n",
    "        for context, y_train in enumerate(y_train_list):\n",
    "            initial_epoch = cycle*len(y_train_list)*epochs + context*epochs\n",
    "            model.set_contexts([context])\n",
    "            model.fit(x_train, y_train, callbacks=callbacks, dynamic_switch=False, initial_epoch=initial_epoch, epochs=initial_epoch + epochs, **kwargs)\n",
    "    \n",
    "    for context in range(len(y_train_list)):\n",
    "        model.set_contexts([context])\n",
    "        tf.print(model.predict(x_train))\n",
    "    \n",
    "    # Calculate and display the accuracy\n",
    "    result = (np.round(model(x_train)).astype(int).flatten() == y_train.flatten()).sum()\n",
    "    print(f\"{result}/{len(y_train)}; Accuracy: {100*result/len(y_train):.2f}%\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training images\n",
    "training_images = idx_load(\"../datasets/mnist/train-images.idx3-ubyte\")\n",
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training labels\n",
    "training_labels = idx_load(\"../datasets/mnist/train-labels.idx1-ubyte\")\n",
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the datasets\n",
    "training_images = training_images.reshape(len(training_images), 28*28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_gate_labels = np.array([\n",
    "    [[0], [1], [1], [0]], # XOR\n",
    "    [[1], [0], [0], [1]], # XNOR\n",
    "    [[0], [0], [0], [1]], # AND\n",
    "    [[0], [1], [1], [1]], # OR\n",
    "    [[1], [0], [0], [0]], # NOR\n",
    "    [[1], [1], [1], [0]], # NAND\n",
    "    [[1], [0], [1], [0]], # Custom 1\n",
    "    [[0], [1], [0], [1]]  # Custom 2\n",
    "])\n",
    "\n",
    "logic_gate_inputs = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST number is even\n",
    "x_train = training_images\n",
    "y_train = np.array([int(i % 2 == 0) for i in training_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[0 1 1 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify on the first 10 the dataset seems correct...\n",
    "print(training_labels[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6916\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6590\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6337\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6120\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5925\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5748\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5585\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5436\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5299\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5172\n",
      "48394/60000; Accuracy: 80.66%\n",
      "13.526658296585083\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "test(Model, x_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictWrapper({})\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6916\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6590\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6337\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6120\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5925\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5748\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5585\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5436\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5299\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5172\n",
      "48394/60000; Accuracy: 80.66%\n",
      "CPU times: user 28.2 s, sys: 16.2 s, total: 44.4 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%time test(NTaskModel, {}, {'accumulate_gradients': False}, x_train, y_train, epochs=10, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/4; Accuracy: 50.00%\n",
      "CPU times: user 3.06 s, sys: 406 ms, total: 3.47 s\n",
      "Wall time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "%time test(NTaskModelBase, {}, {}, logic_gate_inputs, logic_gate_labels[0], epochs=500, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictWrapper({})\n",
      "1/1 [==============================] - 0s 870us/step - loss: 0.7324\n",
      "2/4; Accuracy: 50.00%\n",
      "CPU times: user 250 ms, sys: 15.6 ms, total: 266 ms\n",
      "Wall time: 243 ms\n"
     ]
    }
   ],
   "source": [
    "%time test(NTaskModel, {}, {}, logic_gate_inputs, logic_gate_labels[0], epochs=1, batch_size=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot contexts: [0]\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Context has arguments in `__init__` and therefore must override `get_config`.\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "Executing on task: 0\n",
      "array([[0.12018737],\n",
      "       [0.8882246 ],\n",
      "       [0.8771896 ],\n",
      "       [0.09875786]], dtype=float32)\n",
      "Executing on task: 0\n",
      "4/4; Accuracy: 100.00%\n",
      "CPU times: user 844 ms, sys: 31.2 ms, total: 875 ms\n",
      "Wall time: 826 ms\n"
     ]
    }
   ],
   "source": [
    "%time model = test_context(NTaskModel, {}, {\"metrics\": [tf.keras.metrics.BinaryAccuracy()]}, logic_gate_inputs, logic_gate_labels[:1], cycles=1, epochs=20, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_contexts([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing on task: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4808417 ],\n",
       "       [0.5394176 ],\n",
       "       [0.7024284 ],\n",
       "       [0.33326566]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(logic_gate_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 28162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable(1, trainable=False, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not ResourceVariable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-6e24e1f552ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not ResourceVariable"
     ]
    }
   ],
   "source": [
    "a[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
