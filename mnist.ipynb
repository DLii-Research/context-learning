{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Tasks\n",
    "\n",
    "This notebook utilizes a simple neural network containing a single n-task context layer to perform multiple tasks using the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import Tensorflow and Keras layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Utility functions\n",
    "from utils import idx_load\n",
    "\n",
    "# Import n-task library\n",
    "from ntask.atr   import AtrModel, AtrMovingAverage\n",
    "from ntask.layer import Context\n",
    "from ntask.model import NTaskModel\n",
    "\n",
    "# TEMP: Remove when model prototypes are done\n",
    "from collections import defaultdict\n",
    "import sklearn\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Python modules\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrHarmonicMean(AtrModel):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(Model):\n",
    "    def __init__(self, *args, loss_fn=None, optimizer=None, **kwargs):\n",
    "        super(TestModel, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        #! A temporary way to store things...\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # A list of all context layers\n",
    "        self.context_layers = []\n",
    "        \n",
    "        # Debugging\n",
    "        self.atr_frames = {}\n",
    "        \n",
    "        # Misc\n",
    "        self.total_epochs = 0\n",
    "        \n",
    "        # Create the context layer lookup table\n",
    "        self._create_context_layer_index_list()\n",
    "        \n",
    "        \n",
    "    def compile(self, *args, **kwargs):\n",
    "        super(TestModel, self).compile(*args, **kwargs)\n",
    "        self.create_context_layer_index_list()\n",
    "        \n",
    "    \n",
    "    def _create_context_layer_index_list(self):\n",
    "        \"\"\"\n",
    "        Create a lookup table for context layers.\n",
    "        This is required to efficiently update each context layer's loss information\n",
    "        \"\"\"\n",
    "        self.context_layers = [i for i, layer in enumerate(self.layers) if isinstance(layer, Context)]\n",
    "        self.atr_frames = {i: defaultdict(dict) for i in self.context_layers}\n",
    "        \n",
    "        \n",
    "    def _calc_context_loss(self, context_layer_idx, gradients):\n",
    "        \"\"\"\n",
    "        IMPORTANT: \n",
    "        1) Assumes no use of activation function on Ntask layer\n",
    "        2) Assumes that the layer following the Ntask layer:\n",
    "            a) Is a Dense layer\n",
    "            b) Is using bias \n",
    "               — ex: Dense(20, ... , use_bias=True) \n",
    "               — note Keras Dense layer uses bias by default if no value is given for use_bias param\n",
    "        3) Assumes index of the next layer's gradient is known within the gradients list returned from gradient tape in a tape.gradient call\n",
    "        4) If the above points aren't met, things will break and it may be hard to locate the bugs\n",
    "        \"\"\"\n",
    "        # From the delta rule in neural network math\n",
    "        delta_at_next_layer = gradients[context_layer_idx + 1]\n",
    "        transpose_of_weights_at_next_layer = tf.transpose(self.layers[context_layer_idx + 1].get_weights()[0])\n",
    "        \n",
    "        # Calculate delta at n-task layer\n",
    "        context_delta = np.dot(delta_at_next_layer, transpose_of_weights_at_next_layer).astype(np.float)\n",
    "        \n",
    "        # Calculate Context Error\n",
    "        # Keras MSE must have both args be arrs of floats, if one or both are arrs of ints, the output will be rounded to an int\n",
    "        # This is how responsible the context layer was for the loss\n",
    "        return tf.keras.losses.mean_squared_error(np.zeros(len(context_delta)), context_delta)\n",
    "        \n",
    "    \n",
    "    def _custom_forward_pass(self, x_train, y_train, epoch_grads, batch_size):\n",
    "        \"\"\"\n",
    "        This is the training forward pass for an entire epoch\n",
    "\n",
    "        !!!!! Read through this code as it is a nonstandard training forward pass ( different than model.fit() )\n",
    "        & NOTE that this does not apply the gradients ie. this does not do a weight update/learn\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        grads = None\n",
    "        \n",
    "        # Calculate the total number of batches that need to be processed\n",
    "        num_batches = int(np.ceil(len(x_train) / batch_size))\n",
    "        \n",
    "        # Tensorflow 2 style training -- info can be found here: https://www.tensorflow.org/guide/effective_tf2 \n",
    "        # This is similar to model.fit(), however this is a custom training loop -- ie. it does things differently than model.fit()\n",
    "        # look at each input and label (there are 4 for the logic gates)\n",
    "        for start, end in ((s*batch_size, (s + 1)*batch_size) for s in range(num_batches)):\n",
    "            \n",
    "            # Slice into batch\n",
    "            x = x_train[start:end]\n",
    "            y = y_train[start:end]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self(x, training=True) # Forward pass\n",
    "                loss = self.loss_fn(y, predictions) # Get the loss\n",
    "            \n",
    "            # Extract the gradients for the loss of the current sample\n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            \n",
    "            if grads is None:\n",
    "                grads = gradients\n",
    "            else:\n",
    "                grads = np.add(grads, gradients)\n",
    "            \n",
    "            # Collect the gradients from each sample in the dataset for the epoch\n",
    "#             epoch_grads.append(gradients)\n",
    "#             self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "            \n",
    "            for context_layer_idx in self.context_layers:\n",
    "                self.layers[context_layer_idx].context_loss += self._calc_context_loss(context_layer_idx, gradients)\n",
    "                \n",
    "#         return np.divide(grads, num_batches)\n",
    "        return grads\n",
    "                \n",
    "        \n",
    "    def fit(self, x_train, y_train, n_epochs, shuffle=True, progress=False, batch_size=None, verbose=1):\n",
    "        \n",
    "        # Determine the default batch size\n",
    "        if batch_size is None:\n",
    "            batch_size = len(x_train)\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        x_train, y_train = sklearn.utils.shuffle(x_train, y_train)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            # initialize the values for the loop\n",
    "            epoch_grads = []\n",
    "            \n",
    "            # Backup the weights\n",
    "            weights = self.get_weights()\n",
    "            \n",
    "            # Perform a forward pass\n",
    "            grads = self._custom_forward_pass(x_train, y_train, epoch_grads, batch_size)\n",
    "            \n",
    "            # If no task switch occurred, we can update the weights of the network\n",
    "            self.total_epochs += 1\n",
    "#             for grads in epoch_grads:\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "                        \n",
    "            # Reshuffle the dataset\n",
    "            x_train, y_train = sklearn.utils.shuffle(x_train, y_train)\n",
    "                        \n",
    "                        \n",
    "    def get_contexts(self):\n",
    "        \"\"\"Get the hot context from all context layers\"\"\"\n",
    "        return [self.layers[i].get_hot_context() for i in self.context_layers]\n",
    "            \n",
    "                    \n",
    "    def plot_atr_values(self):\n",
    "        for idx in self.context_layers:\n",
    "            n = self.layers[idx].num_tasks\n",
    "            plotFrames(f\"ATR Values for Context Layer {idx}\", *self.atr_frames[idx].values(), labels=[i for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTaskModel2(Model):\n",
    "    def __init__(self, *args, loss_fn=None, optimizer=None, **kwargs):\n",
    "        super(NTaskModel2, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        #! A temporary way to store things...\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # A list of all context layers\n",
    "        self.context_layers = []\n",
    "        \n",
    "        # Debugging\n",
    "        self.atr_frames = {}\n",
    "        \n",
    "        # Misc\n",
    "        self.total_epochs = 0\n",
    "        \n",
    "        # Create the context layer lookup table\n",
    "        self._create_context_layer_index_list()\n",
    "        \n",
    "        \n",
    "    def compile(self, *args, **kwargs):\n",
    "        super(NTaskModel2, self).compile(*args, **kwargs)\n",
    "        self.create_context_layer_index_list()\n",
    "        \n",
    "    \n",
    "    def _create_context_layer_index_list(self):\n",
    "        \"\"\"\n",
    "        Create a lookup table for context layers.\n",
    "        This is required to efficiently update each context layer's loss information\n",
    "        \"\"\"\n",
    "        self.context_layers = [i for i, layer in enumerate(self.layers) if isinstance(layer, Context)]\n",
    "        self.atr_frames = {i: defaultdict(dict) for i in self.context_layers}\n",
    "        \n",
    "        \n",
    "    def _calc_context_loss(self, context_layer_idx, gradients):\n",
    "        \"\"\"\n",
    "        IMPORTANT: \n",
    "        1) Assumes no use of activation function on Ntask layer\n",
    "        2) Assumes that the layer following the Ntask layer:\n",
    "            a) Is a Dense layer\n",
    "            b) Is using bias \n",
    "               — ex: Dense(20, ... , use_bias=True) \n",
    "               — note Keras Dense layer uses bias by default if no value is given for use_bias param\n",
    "        3) Assumes index of the next layer's gradient is known within the gradients list returned from gradient tape in a tape.gradient call\n",
    "        4) If the above points aren't met, things will break and it may be hard to locate the bugs\n",
    "        \"\"\"\n",
    "        # From the delta rule in neural network math\n",
    "        delta_at_next_layer = gradients[context_layer_idx + 1]\n",
    "        transpose_of_weights_at_next_layer = tf.transpose(self.layers[context_layer_idx + 1].get_weights()[0])\n",
    "        \n",
    "        # Calculate delta at n-task layer\n",
    "        context_delta = np.dot(delta_at_next_layer, transpose_of_weights_at_next_layer).astype(np.float)\n",
    "        \n",
    "        # Calculate Context Error\n",
    "        # Keras MSE must have both args be arrs of floats, if one or both are arrs of ints, the output will be rounded to an int\n",
    "        # This is how responsible the context layer was for the loss\n",
    "        return tf.keras.losses.mean_squared_error(np.zeros(len(context_delta)), context_delta)\n",
    "        \n",
    "    \n",
    "    def _custom_forward_pass(self, x_train, y_train, batch_size):\n",
    "        \"\"\"\n",
    "        This is the training forward pass for an entire epoch\n",
    "\n",
    "        !!!!! Read through this code as it is a nonstandard training forward pass ( different than model.fit() )\n",
    "        & NOTE that this does not apply the gradients ie. this does not do a weight update/learn\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        grads = None\n",
    "        \n",
    "        # Calculate the total number of batches that need to be processed\n",
    "        num_batches = int(np.ceil(len(x_train) / batch_size))\n",
    "        \n",
    "        # Tensorflow 2 style training -- info can be found here: https://www.tensorflow.org/guide/effective_tf2 \n",
    "        # This is similar to model.fit(), however this is a custom training loop -- ie. it does things differently than model.fit()\n",
    "        # look at each input and label (there are 4 for the logic gates)\n",
    "        for start, end in ((s*batch_size, (s + 1)*batch_size) for s in range(num_batches)):\n",
    "            \n",
    "            # Slice into batch\n",
    "            x = x_train[start:end]\n",
    "            y = y_train[start:end]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self(x, training=True) # Forward pass\n",
    "                loss = self.loss_fn(y, predictions) # Get the loss\n",
    "            \n",
    "            # Extract the gradients for the loss of the current sample\n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            \n",
    "            # Add up the total gradients\n",
    "            if grads is None:\n",
    "                grads = gradients\n",
    "            else:\n",
    "                grads = np.add(grads, gradients)\n",
    "            \n",
    "            for context_layer_idx in self.context_layers:\n",
    "                self.layers[context_layer_idx].context_loss += self._calc_context_loss(context_layer_idx, gradients)\n",
    "                \n",
    "        return grads\n",
    "                \n",
    "        \n",
    "    def fit(self, x_train, y_train, n_epochs, shuffle=True, progress=False, explicit_contexts=None, batch_size=None, verbose=1):\n",
    "        \n",
    "        # Explicit context learning: specify the contexts for ecah of the layers. None=dynamic\n",
    "        if explicit_contexts is not None:\n",
    "            if len(explicit_contexts) != len(self.context_layers):\n",
    "                raise ValueError(\"Length of explicit contexts does not match the number of context layers\")\n",
    "            for i, idx in enumerate(self.context_layers):\n",
    "                if explicit_contexts[i] is not None:\n",
    "                    self.layers[idx].set_hot_context(explicit_contexts[i])\n",
    "        else:\n",
    "            explicit_contexts = [None for x in self.context_layers]\n",
    "        \n",
    "        # Determine the default batch size\n",
    "        if batch_size is None:\n",
    "            batch_size = len(x_train)\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        x_train, y_train = sklearn.utils.shuffle(x_train, y_train)\n",
    "        \n",
    "        epoch = 0\n",
    "        while epoch < n_epochs:\n",
    "            \n",
    "            # DEBUG; Display progress\n",
    "            if progress:\n",
    "                display_progress(epoch / n_epochs, title=str([self.layers[i].get_hot_context() for i in self.context_layers]))\n",
    "            \n",
    "            # Reset the context loss\n",
    "            for idx in self.context_layers:\n",
    "                self.layers[idx].context_loss = 0.0\n",
    "            \n",
    "            # Perform a forward pass\n",
    "            grads = self._custom_forward_pass(x_train, y_train, batch_size)\n",
    "            \n",
    "            # Iterate backwards over the context layers. If a context switch occurs, don't check any other layers\n",
    "            switched = False\n",
    "            for i in range(len(self.context_layers) - 1, -1, -1):\n",
    "                # Fetch the context layer\n",
    "                context = self.layers[self.context_layers[i]]\n",
    "                \n",
    "                # Check if explicit context learning for this layer is set\n",
    "                dynamic_switch = explicit_contexts[i] is None\n",
    "                \n",
    "                # Update the layer and indicate if a task switch occurred\n",
    "                if context.update_and_switch(dynamic_switch, verbose=verbose) & Context.RESULT_SWITCHED:\n",
    "                    # A task switch occurred, don't update any other layers/weights\n",
    "                    switched = True\n",
    "                    break\n",
    "            \n",
    "            # If no task switch occurred, we can update the weights of the network\n",
    "            if not switched:\n",
    "                epoch += 1\n",
    "                self.total_epochs += 1\n",
    "                \n",
    "                # Apply the gradients\n",
    "                self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "                for idx in self.context_layers[::-1]:\n",
    "                    for t in range(self.layers[idx].num_tasks):\n",
    "                        self.atr_frames[idx][t][self.total_epochs] = self.layers[idx].atr_model.atr_values[t]\n",
    "                        \n",
    "                # Reshuffle the dataset\n",
    "                x_train, y_train = sklearn.utils.shuffle(x_train, y_train)\n",
    "                        \n",
    "                        \n",
    "    def get_contexts(self):\n",
    "        \"\"\"Get the hot context from all context layers\"\"\"\n",
    "        return [self.layers[i].get_hot_context() for i in self.context_layers]\n",
    "            \n",
    "                    \n",
    "    def plot_atr_values(self):\n",
    "        for idx in self.context_layers:\n",
    "            n = self.layers[idx].num_tasks\n",
    "            plotFrames(f\"ATR Values for Context Layer {idx}\", *self.atr_frames[idx].values(), labels=[i for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training images\n",
    "training_images = idx_load(\"datasets/mnist/train-images.idx3-ubyte\")\n",
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training labels\n",
    "training_labels = idx_load(\"datasets/mnist/train-labels.idx1-ubyte\")\n",
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing images\n",
    "testing_images = idx_load(\"datasets/mnist/t10k-images.idx3-ubyte\")\n",
    "testing_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing labels\n",
    "testing_labels = idx_load(\"datasets/mnist/t10k-labels.idx1-ubyte\")\n",
    "testing_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the datasets\n",
    "training_images = training_images.reshape(len(training_images), 28*28) / 255.0\n",
    "testing_images  = testing_images.reshape(len(testing_images), 28*28) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(labels, predictions):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true=labels, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Simple\n",
    "\n",
    "This model demonstrates a basic model to learn the MNIST dataset.\n",
    "\n",
    "This model successfully learns the MNIST dataset using SGD with a learn rate of ~1e-1 or Adam with 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training_images[:1000]\n",
    "y_train = training_labels[:1000]\n",
    "\n",
    "x_test = testing_images\n",
    "y_test = testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((28*28))\n",
    "# x = Flatten()(inp)\n",
    "x = Dense(128, activation=\"relu\")(inp)\n",
    "x = Dense(10, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6b3bce9810>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883/1000; Accuracy: 88.30%\n"
     ]
    }
   ],
   "source": [
    "result = (np.argmax(model.predict(x_train), axis=1) == y_train).sum()\n",
    "print(f\"{result}/{len(y_train)}; Accuracy: {100*result/len(y_train):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7845/10000; Accuracy: 78.45%\n"
     ]
    }
   ],
   "source": [
    "result = (np.argmax(model.predict(x_test), axis=1) == y_test).sum()\n",
    "print(f\"{result}/{len(y_test)}; Accuracy: {100*result/len(y_test):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: n-task Model\n",
    "\n",
    "This section demonstrates the learning capability of the n-task model without any context layers or switching mechanisms. The architecture is equivalent to that of the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training_images[:1000]\n",
    "y_train = training_labels[:1000]\n",
    "\n",
    "x_test = testing_images\n",
    "y_test = testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((28*28,))\n",
    "# x = Flatten()(inp)\n",
    "x = Dense(128, activation=\"relu\")(inp)\n",
    "x = Context(AtrMovingAverage(1, task_switch_threshold=-0.02))(x)\n",
    "x = Dense(10, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NTaskModel2(inputs=inp, outputs=x, loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, 100, verbose=0, batch_size=32, explicit_contexts=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972/1000; Accuracy: 97.20%\n"
     ]
    }
   ],
   "source": [
    "result = (np.argmax(model.predict(x_train), axis=1) == y_train).sum()\n",
    "print(f\"{result}/{len(y_train)}; Accuracy: {100*result/len(y_train):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8735/10000; Accuracy: 87.35%\n"
     ]
    }
   ],
   "source": [
    "result = (np.argmax(model.predict(x_test), axis=1) == y_test).sum()\n",
    "print(f\"{result}/{len(y_test)}; Accuracy: {100*result/len(y_test):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: n-task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training_images[:1000]\n",
    "y_train = training_labels[:1000]\n",
    "\n",
    "y1_train = np.array([int(i % 3 == 0) for i in y_train]) # Divisible by 3\n",
    "y2_train = np.array([int(i % 2 == 0) for i in y_train]) # Even\n",
    "y3_train = np.array([int(i % 2 == 1) for i in y_train]) # Odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = testing_images\n",
    "y_test = testing_labels\n",
    "\n",
    "y1_test = np.array([int(i % 3 == 0) for i in y_test]) # Divisible by 3\n",
    "y2_test = np.array([int(i % 2 == 0) for i in y_test]) # Even\n",
    "y3_test = np.array([int(i % 2 == 1) for i in y_test]) # Odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5)\n",
    "tf.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = Input((28,28))\n",
    "# x = Flatten()(inp)\n",
    "inp = Input((28*28,))\n",
    "x = Dense(2048, activation=\"relu\")(inp)\n",
    "x = Context(AtrMovingAverage(num_tasks=1, task_switch_threshold=-0.02))(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NTaskModel2(inputs=inp, outputs=x, loss_fn=loss_fn, optimizer=optimizer)\n",
    "# model = Model(inputs=inp, outputs=x)\n",
    "# model.compile(loss=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for cycle in range(1):\n",
    "    for i, y in enumerate([y1_train]):\n",
    "        print(i)\n",
    "        model.fit(x_train, y, 100, verbose=0, batch_size=32, explicit_contexts=[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36761928],\n",
       "       [0.4199247 ],\n",
       "       [0.40151417],\n",
       "       [0.37263757],\n",
       "       [0.36686993],\n",
       "       [0.36947942],\n",
       "       [0.39468235],\n",
       "       [0.4219528 ],\n",
       "       [0.38158232],\n",
       "       [0.36529696],\n",
       "       [0.38845012],\n",
       "       [0.4080037 ],\n",
       "       [0.38494495],\n",
       "       [0.42446116],\n",
       "       [0.37241495],\n",
       "       [0.38248748],\n",
       "       [0.38081726],\n",
       "       [0.3538569 ],\n",
       "       [0.4631479 ],\n",
       "       [0.41913682],\n",
       "       [0.38174725],\n",
       "       [0.41149223],\n",
       "       [0.43985212],\n",
       "       [0.37703764],\n",
       "       [0.4001965 ],\n",
       "       [0.369301  ],\n",
       "       [0.38105702],\n",
       "       [0.41085693],\n",
       "       [0.3723886 ],\n",
       "       [0.43257394],\n",
       "       [0.38811737],\n",
       "       [0.36603785],\n",
       "       [0.46171194],\n",
       "       [0.3521932 ],\n",
       "       [0.3909344 ],\n",
       "       [0.40374854],\n",
       "       [0.35123712],\n",
       "       [0.43378812],\n",
       "       [0.3945792 ],\n",
       "       [0.4231755 ],\n",
       "       [0.35725325],\n",
       "       [0.35135645],\n",
       "       [0.39940447],\n",
       "       [0.4050997 ],\n",
       "       [0.41596556],\n",
       "       [0.38244992],\n",
       "       [0.39590204],\n",
       "       [0.41638902],\n",
       "       [0.37440324],\n",
       "       [0.3853284 ],\n",
       "       [0.4161676 ],\n",
       "       [0.42688876],\n",
       "       [0.38223225],\n",
       "       [0.41431594],\n",
       "       [0.37589455],\n",
       "       [0.3784125 ],\n",
       "       [0.3921925 ],\n",
       "       [0.4119519 ],\n",
       "       [0.33013454],\n",
       "       [0.37113014],\n",
       "       [0.3503536 ],\n",
       "       [0.41719887],\n",
       "       [0.40353596],\n",
       "       [0.4245866 ],\n",
       "       [0.41016018],\n",
       "       [0.39186627],\n",
       "       [0.39848417],\n",
       "       [0.39661604],\n",
       "       [0.4035122 ],\n",
       "       [0.38124743],\n",
       "       [0.37317336],\n",
       "       [0.36226177],\n",
       "       [0.4085104 ],\n",
       "       [0.41299087],\n",
       "       [0.38403603],\n",
       "       [0.4010806 ],\n",
       "       [0.40025327],\n",
       "       [0.37258208],\n",
       "       [0.37346742],\n",
       "       [0.37866628],\n",
       "       [0.3783033 ],\n",
       "       [0.39430308],\n",
       "       [0.39704117],\n",
       "       [0.44267273],\n",
       "       [0.40472832],\n",
       "       [0.3547285 ],\n",
       "       [0.3567865 ],\n",
       "       [0.399394  ],\n",
       "       [0.4281057 ],\n",
       "       [0.4157714 ],\n",
       "       [0.42400894],\n",
       "       [0.3418461 ],\n",
       "       [0.41794392],\n",
       "       [0.40135768],\n",
       "       [0.36452773],\n",
       "       [0.36491102],\n",
       "       [0.36762977],\n",
       "       [0.38386708],\n",
       "       [0.3902182 ],\n",
       "       [0.38365716],\n",
       "       [0.38679317],\n",
       "       [0.38341904],\n",
       "       [0.38921267],\n",
       "       [0.39488342],\n",
       "       [0.3915005 ],\n",
       "       [0.36226362],\n",
       "       [0.40287066],\n",
       "       [0.4113351 ],\n",
       "       [0.3895347 ],\n",
       "       [0.38762113],\n",
       "       [0.39828965],\n",
       "       [0.36732128],\n",
       "       [0.37247044],\n",
       "       [0.36216706],\n",
       "       [0.36760992],\n",
       "       [0.39315876],\n",
       "       [0.37409714],\n",
       "       [0.36763617],\n",
       "       [0.40426397],\n",
       "       [0.38936687],\n",
       "       [0.3668096 ],\n",
       "       [0.38760167],\n",
       "       [0.32999104],\n",
       "       [0.34881425],\n",
       "       [0.40793517],\n",
       "       [0.35805187],\n",
       "       [0.3861122 ],\n",
       "       [0.36865696],\n",
       "       [0.34867728],\n",
       "       [0.37543914],\n",
       "       [0.37681508],\n",
       "       [0.38669038],\n",
       "       [0.3871909 ],\n",
       "       [0.37415522],\n",
       "       [0.35962212],\n",
       "       [0.37037218],\n",
       "       [0.39624155],\n",
       "       [0.34512752],\n",
       "       [0.38642251],\n",
       "       [0.37489402],\n",
       "       [0.39121178],\n",
       "       [0.38003862],\n",
       "       [0.39359266],\n",
       "       [0.37055662],\n",
       "       [0.3928174 ],\n",
       "       [0.3959614 ],\n",
       "       [0.3915202 ],\n",
       "       [0.37901878],\n",
       "       [0.37967688],\n",
       "       [0.36881527],\n",
       "       [0.36146018],\n",
       "       [0.4039563 ],\n",
       "       [0.3816781 ],\n",
       "       [0.43156046],\n",
       "       [0.39382556],\n",
       "       [0.3906792 ],\n",
       "       [0.40306634],\n",
       "       [0.4011615 ],\n",
       "       [0.39559895],\n",
       "       [0.40037283],\n",
       "       [0.41096038],\n",
       "       [0.38583916],\n",
       "       [0.40988076],\n",
       "       [0.391205  ],\n",
       "       [0.37622887],\n",
       "       [0.3896723 ],\n",
       "       [0.38693023],\n",
       "       [0.42654252],\n",
       "       [0.3359362 ],\n",
       "       [0.3845074 ],\n",
       "       [0.38599253],\n",
       "       [0.36980414],\n",
       "       [0.35453808],\n",
       "       [0.3826717 ],\n",
       "       [0.396628  ],\n",
       "       [0.3817641 ],\n",
       "       [0.39090258],\n",
       "       [0.36343592],\n",
       "       [0.37434056],\n",
       "       [0.39172846],\n",
       "       [0.39571455],\n",
       "       [0.38269737],\n",
       "       [0.4161657 ],\n",
       "       [0.4118944 ],\n",
       "       [0.35597306],\n",
       "       [0.36425644],\n",
       "       [0.39466974],\n",
       "       [0.3651518 ],\n",
       "       [0.36885333],\n",
       "       [0.40178454],\n",
       "       [0.3824098 ],\n",
       "       [0.40549332],\n",
       "       [0.3918603 ],\n",
       "       [0.33052033],\n",
       "       [0.3709986 ],\n",
       "       [0.37430108],\n",
       "       [0.37162668],\n",
       "       [0.3708009 ],\n",
       "       [0.39081842],\n",
       "       [0.387888  ],\n",
       "       [0.34432417],\n",
       "       [0.37723532],\n",
       "       [0.39426926],\n",
       "       [0.42796695],\n",
       "       [0.39047897],\n",
       "       [0.3740015 ],\n",
       "       [0.4134882 ],\n",
       "       [0.3715423 ],\n",
       "       [0.36322433],\n",
       "       [0.4004681 ],\n",
       "       [0.42260712],\n",
       "       [0.35591823],\n",
       "       [0.3878275 ],\n",
       "       [0.34943593],\n",
       "       [0.39849174],\n",
       "       [0.38946223],\n",
       "       [0.36772263],\n",
       "       [0.37179905],\n",
       "       [0.41611117],\n",
       "       [0.40926597],\n",
       "       [0.40282008],\n",
       "       [0.4136994 ],\n",
       "       [0.3484101 ],\n",
       "       [0.37478557],\n",
       "       [0.4146147 ],\n",
       "       [0.41323668],\n",
       "       [0.39121202],\n",
       "       [0.38940552],\n",
       "       [0.38130134],\n",
       "       [0.387573  ],\n",
       "       [0.4066584 ],\n",
       "       [0.39332074],\n",
       "       [0.42290172],\n",
       "       [0.3636369 ],\n",
       "       [0.39776754],\n",
       "       [0.37257552],\n",
       "       [0.3818299 ],\n",
       "       [0.4123087 ],\n",
       "       [0.38297194],\n",
       "       [0.36593658],\n",
       "       [0.34065574],\n",
       "       [0.3894589 ],\n",
       "       [0.39354813],\n",
       "       [0.35902798],\n",
       "       [0.3773605 ],\n",
       "       [0.38812548],\n",
       "       [0.3600948 ],\n",
       "       [0.38747382],\n",
       "       [0.37063134],\n",
       "       [0.40342993],\n",
       "       [0.3956483 ],\n",
       "       [0.3502815 ],\n",
       "       [0.33799055],\n",
       "       [0.34148902],\n",
       "       [0.43436682],\n",
       "       [0.39936998],\n",
       "       [0.3759721 ],\n",
       "       [0.3478278 ],\n",
       "       [0.422349  ],\n",
       "       [0.39598662],\n",
       "       [0.3503436 ],\n",
       "       [0.43039033],\n",
       "       [0.38805586],\n",
       "       [0.3816691 ],\n",
       "       [0.4075812 ],\n",
       "       [0.37069762],\n",
       "       [0.36368856],\n",
       "       [0.39823732],\n",
       "       [0.4408012 ],\n",
       "       [0.3970557 ],\n",
       "       [0.40702987],\n",
       "       [0.39291775],\n",
       "       [0.39117807],\n",
       "       [0.38804924],\n",
       "       [0.37208116],\n",
       "       [0.3633167 ],\n",
       "       [0.41600624],\n",
       "       [0.35693872],\n",
       "       [0.40906662],\n",
       "       [0.4365709 ],\n",
       "       [0.40124774],\n",
       "       [0.36746225],\n",
       "       [0.40526274],\n",
       "       [0.38358945],\n",
       "       [0.41115177],\n",
       "       [0.38274348],\n",
       "       [0.34933445],\n",
       "       [0.369268  ],\n",
       "       [0.36641812],\n",
       "       [0.38211536],\n",
       "       [0.37423122],\n",
       "       [0.40033966],\n",
       "       [0.38307744],\n",
       "       [0.34253287],\n",
       "       [0.36376897],\n",
       "       [0.38883826],\n",
       "       [0.41082585],\n",
       "       [0.35488707],\n",
       "       [0.38288385],\n",
       "       [0.4015705 ],\n",
       "       [0.3558728 ],\n",
       "       [0.40428662],\n",
       "       [0.3969022 ],\n",
       "       [0.40210697],\n",
       "       [0.45397833],\n",
       "       [0.4010997 ],\n",
       "       [0.4234104 ],\n",
       "       [0.401285  ],\n",
       "       [0.34308878],\n",
       "       [0.39993048],\n",
       "       [0.39246333],\n",
       "       [0.41322145],\n",
       "       [0.36737153],\n",
       "       [0.42478567],\n",
       "       [0.38980383],\n",
       "       [0.3634132 ],\n",
       "       [0.42232087],\n",
       "       [0.35097003],\n",
       "       [0.38925827],\n",
       "       [0.39571643],\n",
       "       [0.39443558],\n",
       "       [0.3789252 ],\n",
       "       [0.39337587],\n",
       "       [0.40792203],\n",
       "       [0.3693682 ],\n",
       "       [0.36723894],\n",
       "       [0.4028368 ],\n",
       "       [0.36515337],\n",
       "       [0.3868602 ],\n",
       "       [0.34864497],\n",
       "       [0.40465185],\n",
       "       [0.41471392],\n",
       "       [0.40305823],\n",
       "       [0.3332585 ],\n",
       "       [0.3907108 ],\n",
       "       [0.36881104],\n",
       "       [0.34992808],\n",
       "       [0.37544805],\n",
       "       [0.3643422 ],\n",
       "       [0.35235232],\n",
       "       [0.41179585],\n",
       "       [0.39939317],\n",
       "       [0.3722445 ],\n",
       "       [0.38769758],\n",
       "       [0.40492743],\n",
       "       [0.36527333],\n",
       "       [0.40484133],\n",
       "       [0.35468513],\n",
       "       [0.3453338 ],\n",
       "       [0.41325954],\n",
       "       [0.39103237],\n",
       "       [0.35506117],\n",
       "       [0.40006733],\n",
       "       [0.38546833],\n",
       "       [0.3723784 ],\n",
       "       [0.34520042],\n",
       "       [0.3625738 ],\n",
       "       [0.35504943],\n",
       "       [0.3690339 ],\n",
       "       [0.41679287],\n",
       "       [0.36065894],\n",
       "       [0.4101827 ],\n",
       "       [0.40301573],\n",
       "       [0.39318123],\n",
       "       [0.38365632],\n",
       "       [0.37409484],\n",
       "       [0.40579548],\n",
       "       [0.4102587 ],\n",
       "       [0.41135997],\n",
       "       [0.34970152],\n",
       "       [0.34063363],\n",
       "       [0.40696576],\n",
       "       [0.4096189 ],\n",
       "       [0.38905573],\n",
       "       [0.37662026],\n",
       "       [0.36816385],\n",
       "       [0.39754468],\n",
       "       [0.3475405 ],\n",
       "       [0.40854025],\n",
       "       [0.39393136],\n",
       "       [0.39827383],\n",
       "       [0.38434333],\n",
       "       [0.35637814],\n",
       "       [0.41479298],\n",
       "       [0.40150288],\n",
       "       [0.3390053 ],\n",
       "       [0.37209415],\n",
       "       [0.39413393],\n",
       "       [0.39368302],\n",
       "       [0.41588834],\n",
       "       [0.36070043],\n",
       "       [0.35369244],\n",
       "       [0.3827188 ],\n",
       "       [0.35415238],\n",
       "       [0.37653893],\n",
       "       [0.4313591 ],\n",
       "       [0.41928065],\n",
       "       [0.37535894],\n",
       "       [0.3875095 ],\n",
       "       [0.41287088],\n",
       "       [0.4159661 ],\n",
       "       [0.39848447],\n",
       "       [0.3908705 ],\n",
       "       [0.4236382 ],\n",
       "       [0.4210071 ],\n",
       "       [0.40641245],\n",
       "       [0.39516455],\n",
       "       [0.40991834],\n",
       "       [0.38932467],\n",
       "       [0.40685678],\n",
       "       [0.40092406],\n",
       "       [0.39500403],\n",
       "       [0.39333606],\n",
       "       [0.37895662],\n",
       "       [0.39824158],\n",
       "       [0.37108713],\n",
       "       [0.3696848 ],\n",
       "       [0.37922347],\n",
       "       [0.42699984],\n",
       "       [0.41530392],\n",
       "       [0.42862254],\n",
       "       [0.39143723],\n",
       "       [0.3712148 ],\n",
       "       [0.4222131 ],\n",
       "       [0.3868953 ],\n",
       "       [0.42258108],\n",
       "       [0.3709368 ],\n",
       "       [0.39246494],\n",
       "       [0.3835413 ],\n",
       "       [0.388159  ],\n",
       "       [0.41994327],\n",
       "       [0.41023076],\n",
       "       [0.44381028],\n",
       "       [0.38179597],\n",
       "       [0.41644925],\n",
       "       [0.36117285],\n",
       "       [0.41689175],\n",
       "       [0.3558225 ],\n",
       "       [0.36461404],\n",
       "       [0.39632732],\n",
       "       [0.37706417],\n",
       "       [0.39913794],\n",
       "       [0.35821894],\n",
       "       [0.3782224 ],\n",
       "       [0.3892821 ],\n",
       "       [0.3583668 ],\n",
       "       [0.41649324],\n",
       "       [0.35597432],\n",
       "       [0.37445274],\n",
       "       [0.41618133],\n",
       "       [0.374885  ],\n",
       "       [0.442335  ],\n",
       "       [0.35682064],\n",
       "       [0.4127882 ],\n",
       "       [0.3848834 ],\n",
       "       [0.3724321 ],\n",
       "       [0.36567825],\n",
       "       [0.34992605],\n",
       "       [0.37343878],\n",
       "       [0.42133296],\n",
       "       [0.39332348],\n",
       "       [0.39662284],\n",
       "       [0.37849957],\n",
       "       [0.41578382],\n",
       "       [0.35501406],\n",
       "       [0.37688148],\n",
       "       [0.37591144],\n",
       "       [0.35946694],\n",
       "       [0.410727  ],\n",
       "       [0.36687225],\n",
       "       [0.37058434],\n",
       "       [0.36903584],\n",
       "       [0.39679098],\n",
       "       [0.3901555 ],\n",
       "       [0.40180993],\n",
       "       [0.3928556 ],\n",
       "       [0.3904234 ],\n",
       "       [0.38900584],\n",
       "       [0.3335216 ],\n",
       "       [0.3982063 ],\n",
       "       [0.38070142],\n",
       "       [0.41870224],\n",
       "       [0.3835392 ],\n",
       "       [0.3791211 ],\n",
       "       [0.3929772 ],\n",
       "       [0.39548713],\n",
       "       [0.40230393],\n",
       "       [0.40444398],\n",
       "       [0.37512246],\n",
       "       [0.38948494],\n",
       "       [0.39302605],\n",
       "       [0.40258402],\n",
       "       [0.35447988],\n",
       "       [0.37519345],\n",
       "       [0.3838294 ],\n",
       "       [0.40688437],\n",
       "       [0.38437188],\n",
       "       [0.41152087],\n",
       "       [0.39101335],\n",
       "       [0.40060398],\n",
       "       [0.42116192],\n",
       "       [0.418313  ],\n",
       "       [0.3783778 ],\n",
       "       [0.3553605 ],\n",
       "       [0.37865642],\n",
       "       [0.41867515],\n",
       "       [0.4081667 ],\n",
       "       [0.36434025],\n",
       "       [0.38333786],\n",
       "       [0.38320038],\n",
       "       [0.3695445 ],\n",
       "       [0.36141545],\n",
       "       [0.3534131 ],\n",
       "       [0.39680603],\n",
       "       [0.4123549 ],\n",
       "       [0.34784758],\n",
       "       [0.39044714],\n",
       "       [0.39984992],\n",
       "       [0.36837393],\n",
       "       [0.3899864 ],\n",
       "       [0.39939338],\n",
       "       [0.36342025],\n",
       "       [0.36427802],\n",
       "       [0.39755678],\n",
       "       [0.41974515],\n",
       "       [0.38615555],\n",
       "       [0.40545666],\n",
       "       [0.41920772],\n",
       "       [0.35423797],\n",
       "       [0.39154315],\n",
       "       [0.36364794],\n",
       "       [0.40921986],\n",
       "       [0.36766607],\n",
       "       [0.3880544 ],\n",
       "       [0.4034057 ],\n",
       "       [0.37351665],\n",
       "       [0.4137815 ],\n",
       "       [0.3420246 ],\n",
       "       [0.3855095 ],\n",
       "       [0.3601529 ],\n",
       "       [0.42537278],\n",
       "       [0.38806415],\n",
       "       [0.3818331 ],\n",
       "       [0.39154744],\n",
       "       [0.36873114],\n",
       "       [0.3445655 ],\n",
       "       [0.42552453],\n",
       "       [0.3947019 ],\n",
       "       [0.38993374],\n",
       "       [0.40555984],\n",
       "       [0.3949188 ],\n",
       "       [0.40751544],\n",
       "       [0.41153768],\n",
       "       [0.38929176],\n",
       "       [0.38712147],\n",
       "       [0.36941367],\n",
       "       [0.3358174 ],\n",
       "       [0.36393917],\n",
       "       [0.38236046],\n",
       "       [0.37204275],\n",
       "       [0.396171  ],\n",
       "       [0.38970634],\n",
       "       [0.42198402],\n",
       "       [0.36997586],\n",
       "       [0.36912325],\n",
       "       [0.38309133],\n",
       "       [0.41000313],\n",
       "       [0.35977542],\n",
       "       [0.430636  ],\n",
       "       [0.3936729 ],\n",
       "       [0.38568854],\n",
       "       [0.35624218],\n",
       "       [0.37905836],\n",
       "       [0.36672854],\n",
       "       [0.37790364],\n",
       "       [0.3795556 ],\n",
       "       [0.36584178],\n",
       "       [0.43523058],\n",
       "       [0.42666382],\n",
       "       [0.39204344],\n",
       "       [0.38969225],\n",
       "       [0.3807813 ],\n",
       "       [0.40038627],\n",
       "       [0.35358965],\n",
       "       [0.4141768 ],\n",
       "       [0.3872155 ],\n",
       "       [0.3913316 ],\n",
       "       [0.36069906],\n",
       "       [0.36554074],\n",
       "       [0.39232746],\n",
       "       [0.3803539 ],\n",
       "       [0.3531708 ],\n",
       "       [0.4388747 ],\n",
       "       [0.37789053],\n",
       "       [0.40880102],\n",
       "       [0.36038455],\n",
       "       [0.37417367],\n",
       "       [0.42491382],\n",
       "       [0.4018966 ],\n",
       "       [0.36884287],\n",
       "       [0.3909518 ],\n",
       "       [0.41062504],\n",
       "       [0.39399463],\n",
       "       [0.43050164],\n",
       "       [0.40130824],\n",
       "       [0.3920359 ],\n",
       "       [0.36535096],\n",
       "       [0.4144612 ],\n",
       "       [0.35856104],\n",
       "       [0.37138355],\n",
       "       [0.40489078],\n",
       "       [0.40052658],\n",
       "       [0.39813697],\n",
       "       [0.36978683],\n",
       "       [0.3735267 ],\n",
       "       [0.41732875],\n",
       "       [0.38413495],\n",
       "       [0.37527928],\n",
       "       [0.36921996],\n",
       "       [0.40256953],\n",
       "       [0.37136236],\n",
       "       [0.40434784],\n",
       "       [0.37040877],\n",
       "       [0.39390624],\n",
       "       [0.34973735],\n",
       "       [0.3689415 ],\n",
       "       [0.34390545],\n",
       "       [0.4045175 ],\n",
       "       [0.36302537],\n",
       "       [0.3794623 ],\n",
       "       [0.4092275 ],\n",
       "       [0.40698138],\n",
       "       [0.36043817],\n",
       "       [0.40836155],\n",
       "       [0.4175427 ],\n",
       "       [0.34626275],\n",
       "       [0.39476123],\n",
       "       [0.39405233],\n",
       "       [0.39519662],\n",
       "       [0.40978503],\n",
       "       [0.41005197],\n",
       "       [0.37666497],\n",
       "       [0.37957838],\n",
       "       [0.40560436],\n",
       "       [0.40874094],\n",
       "       [0.360766  ],\n",
       "       [0.38538724],\n",
       "       [0.42334265],\n",
       "       [0.38389155],\n",
       "       [0.38828614],\n",
       "       [0.3752506 ],\n",
       "       [0.36388308],\n",
       "       [0.3706798 ],\n",
       "       [0.4056359 ],\n",
       "       [0.37257612],\n",
       "       [0.38425127],\n",
       "       [0.40728033],\n",
       "       [0.3767727 ],\n",
       "       [0.42069477],\n",
       "       [0.39126787],\n",
       "       [0.41764012],\n",
       "       [0.4133575 ],\n",
       "       [0.40979314],\n",
       "       [0.38917953],\n",
       "       [0.40516764],\n",
       "       [0.38552734],\n",
       "       [0.40590325],\n",
       "       [0.41119814],\n",
       "       [0.4115535 ],\n",
       "       [0.38784692],\n",
       "       [0.40969703],\n",
       "       [0.35088772],\n",
       "       [0.375186  ],\n",
       "       [0.32986414],\n",
       "       [0.4391533 ],\n",
       "       [0.4074857 ],\n",
       "       [0.3876655 ],\n",
       "       [0.35614985],\n",
       "       [0.3709275 ],\n",
       "       [0.3511575 ],\n",
       "       [0.37112963],\n",
       "       [0.39807856],\n",
       "       [0.4307691 ],\n",
       "       [0.37589055],\n",
       "       [0.406731  ],\n",
       "       [0.3447085 ],\n",
       "       [0.38944507],\n",
       "       [0.3824908 ],\n",
       "       [0.36818856],\n",
       "       [0.41505024],\n",
       "       [0.3818525 ],\n",
       "       [0.35150757],\n",
       "       [0.4035372 ],\n",
       "       [0.428415  ],\n",
       "       [0.3478629 ],\n",
       "       [0.3841455 ],\n",
       "       [0.42076996],\n",
       "       [0.35059783],\n",
       "       [0.36660105],\n",
       "       [0.39962003],\n",
       "       [0.3639469 ],\n",
       "       [0.38627478],\n",
       "       [0.39550522],\n",
       "       [0.3797257 ],\n",
       "       [0.3966264 ],\n",
       "       [0.33012614],\n",
       "       [0.4038915 ],\n",
       "       [0.40200874],\n",
       "       [0.38697565],\n",
       "       [0.41002345],\n",
       "       [0.36195877],\n",
       "       [0.36028248],\n",
       "       [0.35076895],\n",
       "       [0.35104296],\n",
       "       [0.39017543],\n",
       "       [0.39995193],\n",
       "       [0.38580152],\n",
       "       [0.34402403],\n",
       "       [0.38291532],\n",
       "       [0.35810396],\n",
       "       [0.3504771 ],\n",
       "       [0.3903564 ],\n",
       "       [0.41009688],\n",
       "       [0.36994928],\n",
       "       [0.38812304],\n",
       "       [0.29960093],\n",
       "       [0.39107674],\n",
       "       [0.36592805],\n",
       "       [0.42410094],\n",
       "       [0.34957486],\n",
       "       [0.3282891 ],\n",
       "       [0.39393926],\n",
       "       [0.32499218],\n",
       "       [0.42083535],\n",
       "       [0.3756246 ],\n",
       "       [0.3819756 ],\n",
       "       [0.36743408],\n",
       "       [0.35408384],\n",
       "       [0.40152338],\n",
       "       [0.3839668 ],\n",
       "       [0.3939764 ],\n",
       "       [0.39577872],\n",
       "       [0.39318037],\n",
       "       [0.37404287],\n",
       "       [0.3843029 ],\n",
       "       [0.39954472],\n",
       "       [0.36915442],\n",
       "       [0.37652442],\n",
       "       [0.4099623 ],\n",
       "       [0.34340382],\n",
       "       [0.36802453],\n",
       "       [0.38744527],\n",
       "       [0.37900543],\n",
       "       [0.36049932],\n",
       "       [0.42491013],\n",
       "       [0.38274145],\n",
       "       [0.33427215],\n",
       "       [0.39709055],\n",
       "       [0.3806382 ],\n",
       "       [0.37668306],\n",
       "       [0.4278697 ],\n",
       "       [0.35528046],\n",
       "       [0.39716157],\n",
       "       [0.36191145],\n",
       "       [0.3905421 ],\n",
       "       [0.36822006],\n",
       "       [0.3704607 ],\n",
       "       [0.40748462],\n",
       "       [0.37509686],\n",
       "       [0.35198784],\n",
       "       [0.4180339 ],\n",
       "       [0.3963647 ],\n",
       "       [0.39026952],\n",
       "       [0.37046313],\n",
       "       [0.38659596],\n",
       "       [0.36903548],\n",
       "       [0.40836266],\n",
       "       [0.36714083],\n",
       "       [0.439649  ],\n",
       "       [0.3752003 ],\n",
       "       [0.34745204],\n",
       "       [0.38164106],\n",
       "       [0.39719814],\n",
       "       [0.3701627 ],\n",
       "       [0.39026603],\n",
       "       [0.37377203],\n",
       "       [0.42214632],\n",
       "       [0.40836066],\n",
       "       [0.40931135],\n",
       "       [0.4084176 ],\n",
       "       [0.37779176],\n",
       "       [0.34942582],\n",
       "       [0.38221937],\n",
       "       [0.40377173],\n",
       "       [0.34662688],\n",
       "       [0.35286957],\n",
       "       [0.40656215],\n",
       "       [0.37198192],\n",
       "       [0.40885305],\n",
       "       [0.37349856],\n",
       "       [0.38078287],\n",
       "       [0.38411617],\n",
       "       [0.40938333],\n",
       "       [0.36983794],\n",
       "       [0.3980555 ],\n",
       "       [0.3838728 ],\n",
       "       [0.38382804],\n",
       "       [0.39232853],\n",
       "       [0.38444695],\n",
       "       [0.3709703 ],\n",
       "       [0.37077892],\n",
       "       [0.38901576],\n",
       "       [0.37085903],\n",
       "       [0.38007456],\n",
       "       [0.31956953],\n",
       "       [0.37974262],\n",
       "       [0.39547324],\n",
       "       [0.3391003 ],\n",
       "       [0.39359054],\n",
       "       [0.39510244],\n",
       "       [0.38526532],\n",
       "       [0.34964907],\n",
       "       [0.43155384],\n",
       "       [0.38262028],\n",
       "       [0.37082633],\n",
       "       [0.38152117],\n",
       "       [0.384672  ],\n",
       "       [0.3735862 ],\n",
       "       [0.37894183],\n",
       "       [0.388622  ],\n",
       "       [0.34852892],\n",
       "       [0.3875671 ],\n",
       "       [0.3858992 ],\n",
       "       [0.36777526],\n",
       "       [0.3797515 ],\n",
       "       [0.36364976],\n",
       "       [0.391887  ],\n",
       "       [0.41131905],\n",
       "       [0.37267104],\n",
       "       [0.37515873],\n",
       "       [0.38752407],\n",
       "       [0.3758732 ],\n",
       "       [0.40237644],\n",
       "       [0.41848364],\n",
       "       [0.3885525 ],\n",
       "       [0.38034323],\n",
       "       [0.40283847],\n",
       "       [0.33474854],\n",
       "       [0.36480844],\n",
       "       [0.42900762],\n",
       "       [0.41238135],\n",
       "       [0.3798434 ],\n",
       "       [0.35706615],\n",
       "       [0.37953734],\n",
       "       [0.38908345],\n",
       "       [0.36223269],\n",
       "       [0.37245482],\n",
       "       [0.36392936],\n",
       "       [0.39307743],\n",
       "       [0.38621604],\n",
       "       [0.40236843],\n",
       "       [0.36565363],\n",
       "       [0.35176733],\n",
       "       [0.3883502 ],\n",
       "       [0.41202676],\n",
       "       [0.37658808],\n",
       "       [0.43389916],\n",
       "       [0.40171868],\n",
       "       [0.38951924],\n",
       "       [0.43868884],\n",
       "       [0.36492437],\n",
       "       [0.373087  ],\n",
       "       [0.4011054 ],\n",
       "       [0.38651335],\n",
       "       [0.37977555],\n",
       "       [0.3624552 ],\n",
       "       [0.3397923 ],\n",
       "       [0.3798221 ],\n",
       "       [0.3910811 ],\n",
       "       [0.37302643],\n",
       "       [0.38727602],\n",
       "       [0.41718447],\n",
       "       [0.38410068],\n",
       "       [0.4124825 ],\n",
       "       [0.37009424],\n",
       "       [0.3593136 ],\n",
       "       [0.3942893 ],\n",
       "       [0.35986903],\n",
       "       [0.3983683 ],\n",
       "       [0.4045285 ],\n",
       "       [0.38289398],\n",
       "       [0.36301982],\n",
       "       [0.40602112],\n",
       "       [0.38100234],\n",
       "       [0.37520278],\n",
       "       [0.3856145 ],\n",
       "       [0.36898082],\n",
       "       [0.39022887],\n",
       "       [0.3810408 ],\n",
       "       [0.3974422 ],\n",
       "       [0.41148365],\n",
       "       [0.3723665 ],\n",
       "       [0.42812473],\n",
       "       [0.38910162],\n",
       "       [0.40609947],\n",
       "       [0.37306648],\n",
       "       [0.42224568],\n",
       "       [0.38380075],\n",
       "       [0.40327612],\n",
       "       [0.38903916],\n",
       "       [0.43288574],\n",
       "       [0.35381067],\n",
       "       [0.3813006 ],\n",
       "       [0.40603477],\n",
       "       [0.39119452],\n",
       "       [0.38660657],\n",
       "       [0.37530977],\n",
       "       [0.3603361 ],\n",
       "       [0.39926273],\n",
       "       [0.38017505],\n",
       "       [0.36483067],\n",
       "       [0.3818272 ],\n",
       "       [0.39389315],\n",
       "       [0.37663347],\n",
       "       [0.4045442 ],\n",
       "       [0.3838553 ],\n",
       "       [0.35960186],\n",
       "       [0.40197617],\n",
       "       [0.4077987 ],\n",
       "       [0.4204217 ],\n",
       "       [0.39794976],\n",
       "       [0.38579226],\n",
       "       [0.39360338],\n",
       "       [0.38962987],\n",
       "       [0.39359203],\n",
       "       [0.4017372 ],\n",
       "       [0.36049914],\n",
       "       [0.41687602],\n",
       "       [0.38425386],\n",
       "       [0.42125672],\n",
       "       [0.40753797],\n",
       "       [0.4018135 ],\n",
       "       [0.35870177],\n",
       "       [0.40634978],\n",
       "       [0.38856104],\n",
       "       [0.39910588],\n",
       "       [0.35812765],\n",
       "       [0.41282487],\n",
       "       [0.38800395],\n",
       "       [0.37534925],\n",
       "       [0.3222105 ],\n",
       "       [0.37568837],\n",
       "       [0.3985846 ],\n",
       "       [0.37713194],\n",
       "       [0.4003791 ],\n",
       "       [0.39557594],\n",
       "       [0.3581655 ],\n",
       "       [0.41076407],\n",
       "       [0.37334657],\n",
       "       [0.34891176],\n",
       "       [0.35229135],\n",
       "       [0.3759856 ],\n",
       "       [0.338297  ],\n",
       "       [0.3480971 ],\n",
       "       [0.40717345],\n",
       "       [0.4229953 ],\n",
       "       [0.38090253],\n",
       "       [0.3917587 ],\n",
       "       [0.3963007 ],\n",
       "       [0.4084413 ],\n",
       "       [0.39171   ],\n",
       "       [0.38669154],\n",
       "       [0.38825768],\n",
       "       [0.38876322],\n",
       "       [0.40646604],\n",
       "       [0.3731938 ],\n",
       "       [0.40672618],\n",
       "       [0.39601836],\n",
       "       [0.37834468],\n",
       "       [0.4022323 ],\n",
       "       [0.39171332],\n",
       "       [0.3438832 ],\n",
       "       [0.37903205],\n",
       "       [0.38951144],\n",
       "       [0.39622915],\n",
       "       [0.37528536],\n",
       "       [0.3895027 ],\n",
       "       [0.4035439 ],\n",
       "       [0.36647016],\n",
       "       [0.41062906],\n",
       "       [0.38846946],\n",
       "       [0.3809662 ],\n",
       "       [0.40647852],\n",
       "       [0.39669666],\n",
       "       [0.39948842],\n",
       "       [0.35936344],\n",
       "       [0.37837726],\n",
       "       [0.38132703],\n",
       "       [0.4008395 ],\n",
       "       [0.39213315]], dtype=float32)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context(0): 97/1000; Accuracy: 9.70%\n"
     ]
    }
   ],
   "source": [
    "for context, y in enumerate([y_train]):\n",
    "#     model.layers[model.context_layers[0]].set_hot_context(context)\n",
    "    result = (np.round(model.predict(x_train).flatten()).astype(int) == y).sum()\n",
    "    print(f\"Context({context}): {result}/{len(y)}; Accuracy: {100*result/len(y):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_atr_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.49238953]], dtype=float32)>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "correct = np.zeros(3)\n",
    "for i, y in enumerate([y1_train, y2_train, y3_train]):\n",
    "    model.layers[2].set_hot_context(i)\n",
    "    result = np.round(model(x_test)).astype(int).flatten()\n",
    "    correct[i] = np.sum(result == y)/len(y)\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_hot_context(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6460 - accuracy: 0.8280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6459522247314453, 0.828000009059906]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate doesn't work. Hmmm...\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[9.9694675e-01, 6.5245154e-10, 1.3429130e-04, 1.3306305e-07,\n",
       "        6.1000824e-06, 7.7564758e-04, 3.3671377e-06, 3.8232724e-06,\n",
       "        2.1278865e-03, 2.0759178e-06]], dtype=float32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(np.array([x_test[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
