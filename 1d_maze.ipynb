{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Maze\n",
    "\n",
    "This reinforcement learning task utilizes the n-task model in a to solve a simple 1D maze. The 1D maze has multiple goals, but the agent told neither where the goal is nor which goal is currently active. The agent must learn to navigate the maze to locate the goal position.\n",
    "\n",
    "![](./graphics/1d_maze.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tensorflow and Keras layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import n-task library\n",
    "from ntask.atr   import AtrModel, AtrMovingAverage\n",
    "from ntask.layer import Context\n",
    "from ntask.model import NTaskModel\n",
    "from ntask.utils import display_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrRl(AtrModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(state, n_states):\n",
    "    a = np.zeros(n_states)\n",
    "    a[state] = 1\n",
    "    return np.array([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, y_pred):\n",
    "    return tf.keras.losses.mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeAgent:\n",
    "    \n",
    "    # Agent actions\n",
    "    ACTION_LEFT  = 0\n",
    "    ACTION_RIGHT = 1\n",
    "    \n",
    "    def __init__(self, size, goals, alpha=1.0, gamma=0.75, epsilon=0.2):\n",
    "        # Maze parameters\n",
    "        self.maze_size = size\n",
    "        self.goals     = goals\n",
    "        \n",
    "        # Agent parameters\n",
    "        self.alpha   = alpha   # Q-learning learning rate\n",
    "        self.gamma   = gamma   # Discount factor\n",
    "        self.epsilon = epsilon # Epsilon-greedy policy random action probability\n",
    "        \n",
    "        # Create the n-task model\n",
    "        self.init_model()\n",
    "        \n",
    "        # Misc\n",
    "        self.episode = 0\n",
    "    \n",
    "    \n",
    "    def init_model(self):\n",
    "        inp = Input((self.maze_size,))\n",
    "#         x = Dense(256, activation=\"relu\")(inp)\n",
    "#         x = Context(AtrMovingAverage(num_tasks=len(self.goals)))(x)\n",
    "        x = Dense(2, activation=\"linear\")(inp)\n",
    "#         self.model = NTaskModel(inputs=inp, outputs=x, loss_fn=loss_fn, optimizer=optimizer)\n",
    "        self.model = Model(inputs=inp, outputs=x)\n",
    "        self.model.compile(loss=loss_fn, optimizer=\"adam\")\n",
    "        \n",
    "        \n",
    "    def predict(self, state):\n",
    "        return self.model.predict(one_hot_encode(state, self.maze_size))[0]\n",
    "        \n",
    "        \n",
    "    def policy(self, state, predicted):\n",
    "        \"\"\"Determine the action given the state\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            index = random.randint(0, 1) # left/right\n",
    "        else:\n",
    "            index = np.argmax(predicted)\n",
    "        return index, predicted\n",
    "    \n",
    "    \n",
    "    def reward(self, state, goal):\n",
    "        return int(state == goal)\n",
    "    \n",
    "    \n",
    "    def next_state(self, state, action):\n",
    "        \"\"\"Calculate the next state in the maze\"\"\"\n",
    "        if action == MazeAgent.ACTION_LEFT:\n",
    "            return (state - 1) % self.maze_size\n",
    "        else:\n",
    "            return (state + 1) % self.maze_size\n",
    "        \n",
    "        \n",
    "    def random_start_state(self, goal):\n",
    "        \"\"\"Generate a random starting state\"\"\"\n",
    "        state = random.randrange(self.maze_size)\n",
    "        while state == goal:\n",
    "            state = random.randrange(self.maze_size)\n",
    "        return state\n",
    "        \n",
    "        \n",
    "    def run(self, goal, limit):\n",
    "        \"\"\"Perform an episode\"\"\"\n",
    "        \n",
    "        self.episode += 1\n",
    "        \n",
    "        # Pick a random starting state that is not the goal position\n",
    "        state = self.random_start_state(goal)\n",
    "        \n",
    "        # Initialize prev state, action, action values\n",
    "        prev_state, action, action_values = None, None, None\n",
    "        \n",
    "        # Count the total number of steps\n",
    "        steps = 0\n",
    "        \n",
    "        # Get the initial prediction\n",
    "        # This allows a caching mechanism so we don't predict on a state twice\n",
    "        prediction = self.predict(state)\n",
    "        \n",
    "        while state != goal:\n",
    "            \n",
    "            # If the step count has been exceeded, terminate with no reward\n",
    "            if steps > limit:\n",
    "                return steps\n",
    "            \n",
    "            # If the agent has moved, update the previous state's value\n",
    "            if prev_state is not None:\n",
    "                \n",
    "                # Grab the prediction values for this new state (cached for the next prediction)\n",
    "                prediction = self.predict(state)\n",
    "                \n",
    "                # Get the highest valued action at this state\n",
    "                action_max = np.max(prediction)\n",
    "                \n",
    "                # Calculate the TD error with a reward of 0\n",
    "                td_error = self.reward(state, goal) + self.gamma*action_max - action_values[action]\n",
    "                \n",
    "                # Update the target values\n",
    "                action_values[action] += self.alpha*td_error\n",
    "                \n",
    "                # Update the model\n",
    "                self.model.fit(one_hot_encode(prev_state, self.maze_size), np.array([action_values]), verbose=0)\n",
    "            \n",
    "            # Determine the action to make from the policy\n",
    "            action, action_values = self.policy(state, prediction)\n",
    "            \n",
    "            # Transition to the next state, remember the previous\n",
    "            prev_state, state = state, self.next_state(state, action)\n",
    "            \n",
    "            # Increment the step            \n",
    "            steps += 1\n",
    "            \n",
    "        # --- At this point the goal has been reached ------------------------\n",
    "        \n",
    "        # Absorb the reward\n",
    "        action_values[action] = self.reward(state, goal)\n",
    "        self.model.fit(one_hot_encode(prev_state, self.maze_size), np.array([action_values]), verbose=0)\n",
    "        \n",
    "        return steps\n",
    "        \n",
    "    \n",
    "    def train(self, episodes, cycles=1, limit=100):\n",
    "        \"\"\"Train the agent\"\"\"\n",
    "        \n",
    "        # Map the goals to context indices\n",
    "        goal_indices = list(range(len(self.goals)))\n",
    "        prev_index = None\n",
    "        \n",
    "        ep = 1\n",
    "        total_episodes = episodes*len(self.goals)*cycles\n",
    "        \n",
    "        for cycle in range(cycles):\n",
    "            \n",
    "            # Shuffle the goals around after first complete cycle\n",
    "            # Ensures that two goals are not repeated\n",
    "            if cycle > 0:\n",
    "                random.shuffle(goal_indices)\n",
    "                if prev_index == goal_indices[0]:\n",
    "                    index = random.randrange(1, len(self.goals))\n",
    "                    goal_indices[0], goal_indices[index] = goal_indices[index], goal_indices[0]\n",
    "            \n",
    "            # Perform #episodes per goal\n",
    "            for goal in goal_indices:\n",
    "                for episode in range(episodes):\n",
    "                    self.run(self.goals[goal_indices[goal]], limit)\n",
    "                    if ep % 1 == 0:\n",
    "                        display_progress(ep / total_episodes, f\"{ep}/{total_episodes}\")\n",
    "                    ep += 1\n",
    "        \n",
    "    \n",
    "    def plot(self):\n",
    "\n",
    "        # All possible one-hot state encodings\n",
    "        states = np.identity(10)\n",
    "        action_labels = (\"left\", \"right\")\n",
    "        \n",
    "        # Create the plot and x-axis increments\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        t = np.arange(0, self.maze_size+1, 1)\n",
    "        \n",
    "        for goal in range(len(self.goals)):\n",
    "            \n",
    "            # Set the model to the goal's context and grab the action values for each state\n",
    "            if self.model isinstance NTaskModel:\n",
    "                self.model.layers[self.model.context_layers[0]].set_hot_context(goal)\n",
    "            values = self.model.predict(states)\n",
    "            \n",
    "            for i, row in enumerate(np.transpose(values)):\n",
    "                ax.plot(t, np.append(row, row[0]), 'o-', label=f\"ctx({goal}): {action_labels[i]}\")\n",
    "            \n",
    "        ax.set(xlabel='Position (s)', ylabel='Value Q(s, a)', title=\"n-task Maze Agent\")\n",
    "        ax.grid()\n",
    "        plt.legend()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MazeAgent(10, [5], alpha=1.0, gamma=0.75, epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/500\n",
      "Progress: [##########----------] 53.20%\n"
     ]
    }
   ],
   "source": [
    "agent.train(500, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
